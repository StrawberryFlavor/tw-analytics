"""
Fast View Booster Service - No Browser Required
High-speed Twitter view boosting using direct HTTP requests
"""

import asyncio
import httpx
import random
import json
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, field

from .proxy_pool import ProxyPool
from .smart_proxy_manager import get_smart_proxy_manager
from account_management import AccountManager
from account_management.models import Account


@dataclass
class FastBoosterConfig:
    """Configuration for fast view booster"""
    target_urls: List[str] = field(default_factory=list)
    target_views: int = 1000
    max_concurrent_requests: int = 10
    request_interval: tuple = (1, 3)  # Random interval between requests (seconds)
    use_proxy_pool: bool = True
    proxy: Optional[str] = None  # Single proxy URL (ignored if use_proxy_pool=True)
    timeout: int = 10  # Request timeout in seconds
    retry_on_failure: bool = True
    max_retries: int = 3


class FastViewBooster:
    """Fast view booster using HTTP requests instead of browser"""
    
    def __init__(self, config: FastBoosterConfig, account_manager: AccountManager):
        self.config = config
        self.account_manager = account_manager
        
        # ä½¿ç”¨æ™ºèƒ½ä»£ç†ç®¡ç†å™¨æ›¿ä»£ç®€å•çš„ä»£ç†æ± 
        self.smart_proxy_manager = get_smart_proxy_manager()
        
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)  # Enable debug logging
        
        # Statistics
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "views_per_url": {url: 0 for url in config.target_urls},
            "start_time": datetime.now()
        }
        
        # Control flags
        self.running = False
        self._stop_event = asyncio.Event()
        
        # Session cache for accounts to avoid repeated logins
        self._session_cache = {}  # {username: {"cookies": {}, "expires": timestamp}}
        self._session_cache_duration = 3600  # 1 hour cache
        
        # User agents pool
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15"
        ]
    
    def _get_headers(self, account: Account) -> Dict[str, str]:
        """Construct request headers to mimic real browser"""
        return {
            "User-Agent": random.choice(self.user_agents),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Accept-Encoding": "gzip, deflate, br",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1",
            "Cache-Control": "max-age=0"
        }
    
    
    async def _login_and_get_session(self, account: Account, client: httpx.AsyncClient) -> Dict[str, str]:
        """é€šè¿‡HTTPè¯·æ±‚ç™»å½•å¹¶è·å–å®Œæ•´çš„session cookies"""
        self.logger.info(f"å¼€å§‹ç™»å½•è´¦æˆ· {account.username}...")
        
        try:
            # 1. é¦–å…ˆè®¿é—®Twitterä¸»é¡µè·å–guest token
            headers = {
                "User-Agent": random.choice(self.user_agents),
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.5",
                "Accept-Encoding": "gzip, deflate, br",
                "DNT": "1",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Cache-Control": "max-age=0"
            }
            
            response = await client.get("https://twitter.com", headers=headers)
            if response.status_code != 200:
                self.logger.error(f"æ— æ³•è®¿é—®Twitterä¸»é¡µ: {response.status_code}")
                self.logger.debug(f"å“åº”å†…å®¹: {response.text[:500]}")
                return {}
            
            # ä»å“åº”ä¸­æå–cookies
            session_cookies = {}
            for name, value in response.cookies.items():
                session_cookies[name] = value
            
            self.logger.debug(f"è·å–åˆ°åˆå§‹cookies: {list(session_cookies.keys())}")
            
            # 2. è®¾ç½®auth_tokenåˆ°cookiesä¸­
            if hasattr(account, 'auth_token') and account.auth_token:
                session_cookies['auth_token'] = account.auth_token
                self.logger.debug(f"è®¾ç½®auth_token: {account.auth_token[:10]}...")
            else:
                self.logger.error(f"è´¦æˆ· {account.username} æ²¡æœ‰auth_token")
                return {}
            
            # 3. è®¿é—®/homeéªŒè¯ç™»å½•çŠ¶æ€å¹¶è·å–CSRF token
            headers = {
                "User-Agent": random.choice(self.user_agents),
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.5",
                "Accept-Encoding": "gzip, deflate, br",
                "Referer": "https://twitter.com/",
                "DNT": "1",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
            }
            
            response = await client.get(
                "https://twitter.com/home",
                cookies=session_cookies,
                headers=headers,
                follow_redirects=True
            )
            
            # æ›´æ–°cookies
            for name, value in response.cookies.items():
                session_cookies[name] = value
            
            self.logger.debug(f"ç™»å½•åcookies: {list(session_cookies.keys())}")
            
            # 4. ä»é¡µé¢HTMLä¸­æå–CSRF token
            if response.status_code == 200:
                html_content = response.text
                
                # æŸ¥æ‰¾é¡µé¢ä¸­çš„CSRF token (å¤šç§å¯èƒ½çš„æ ¼å¼)
                import re
                csrf_patterns = [
                    r'"ct0":"([^"]+)"',                    # JavaScriptä¸­çš„ct0
                    r'ct0=([^;]+)',                       # Cookieæ ¼å¼
                    r'"csrf_token":"([^"]+)"',            # å¦ä¸€ç§æ ¼å¼
                    r'<input[^>]*csrf[^>]*value="([^"]+)"'  # HTMLè¡¨å•ä¸­çš„CSRF
                ]
                
                csrf_token = None
                for pattern in csrf_patterns:
                    csrf_match = re.search(pattern, html_content)
                    if csrf_match:
                        csrf_token = csrf_match.group(1)
                        self.logger.debug(f"ä»é¡µé¢æå–CSRF token (æ¨¡å¼ {pattern[:20]}...): {csrf_token[:10]}...")
                        break
                
                if csrf_token:
                    session_cookies['ct0'] = csrf_token
                else:
                    # å¦‚æœé¡µé¢ä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»ç°æœ‰cookiesä¸­è·å–æˆ–ç”Ÿæˆ
                    if 'ct0' in session_cookies:
                        csrf_token = session_cookies['ct0']
                        self.logger.debug(f"ä½¿ç”¨ç°æœ‰ct0 cookie: {csrf_token[:10]}...")
                    else:
                        # æœ€åæ‰‹æ®µï¼šç”Ÿæˆä¸€ä¸ªéšæœºçš„32å­—ç¬¦token
                        import secrets
                        csrf_token = secrets.token_hex(16)  # 32å­—ç¬¦
                        session_cookies['ct0'] = csrf_token
                        self.logger.debug(f"ç”ŸæˆéšæœºCSRF token: {csrf_token[:10]}...")
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸç™»å½•
                final_url = str(response.url)
                if '/home' in final_url or 'login' not in final_url.lower():
                    self.logger.info(f"âœ… è´¦æˆ· {account.username} ç™»å½•æˆåŠŸ")
                    return session_cookies
                else:
                    self.logger.warning(f"âš ï¸ è´¦æˆ· {account.username} å¯èƒ½æœªæ­£ç¡®ç™»å½•ï¼ŒURL: {final_url}")
                    return session_cookies  # ä»ç„¶è¿”å›cookiesï¼Œå¯èƒ½ä»å¯ä½¿ç”¨
            else:
                self.logger.error(f"ç™»å½•éªŒè¯å¤±è´¥: {response.status_code}")
                return {}
                
        except Exception as e:
            self.logger.error(f"ç™»å½•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ {account.username}: {str(e)}")
            return {}
    
    async def _get_or_refresh_session(self, account: Account, client: httpx.AsyncClient) -> Dict[str, str]:
        """è·å–æˆ–åˆ·æ–°è´¦æˆ·çš„sessionï¼Œä½¿ç”¨ç¼“å­˜æœºåˆ¶"""
        username = account.username
        current_time = datetime.now().timestamp()
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        if username in self._session_cache:
            cached_session = self._session_cache[username]
            if cached_session["expires"] > current_time:
                self.logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„session for {username}")
                return cached_session["cookies"]
            else:
                self.logger.debug(f"Sessionç¼“å­˜å·²è¿‡æœŸ for {username}")
        
        # ç¼“å­˜æ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œé‡æ–°ç™»å½•
        self.logger.info(f"åˆ·æ–°session for {username}")
        cookies = await self._login_and_get_session(account, client)
        
        if cookies:
            # ç¼“å­˜æ–°çš„session
            self._session_cache[username] = {
                "cookies": cookies,
                "expires": current_time + self._session_cache_duration
            }
            self.logger.debug(f"Sessionå·²ç¼“å­˜ for {username}ï¼Œæœ‰æ•ˆæœŸåˆ° {datetime.fromtimestamp(current_time + self._session_cache_duration)}")
        
        return cookies
    
    def _parse_cookies(self, account: Account) -> Dict[str, str]:
        """Parse cookies from account data - use auth_token (deprecated, use _login_and_get_session instead)"""
        cookies = {}
        
        # Use auth_token as cookie for authentication
        if hasattr(account, 'auth_token') and account.auth_token:
            # Generate a stable CSRF token based on auth_token hash for consistency
            import hashlib
            auth_hash = hashlib.md5(account.auth_token.encode()).hexdigest()
            csrf_token = auth_hash  # Use MD5 hash as CSRF token (32 chars)
            
            cookies = {
                'auth_token': account.auth_token,
                'ct0': csrf_token,  # Must match x-csrf-token header
                'guest_id': f'v1%3A{int(datetime.now().timestamp() * 1000)}',
                'personalization_id': f'"v1_{int(datetime.now().timestamp() * 1000)}"',
                'lang': 'en',
                '_twitter_sess': 'BAh7CSIKZmxhc2hJQzonQWN0aW9uQ29udHJvbGxlcjo6Rmxhc2g6OkZsYXNo%250ASGFzaHsABjoKQHVzZWR7ADoPY3JlYXRlZF9hdGwrCECwlcN5AToMY3NyZl9p%250AZCIlNjE2MTYxNjE2MTYxNjE2MTYxNjE2MTYxNjE2MTYxNjE2MTYxNjE2MTYx%250AZGI0ZjZmNzIzNDIzNDIzNDIzNDIzNDIzNDIzNA%253D%253D--1234567890abcdef'
            }
            
            self.logger.debug(f"Using auth_token for {account.username}: {account.auth_token[:10]}...")
            self.logger.debug(f"Generated CSRF token: {csrf_token[:10]}...")
        else:
            self.logger.warning(f"No auth_token for account {account.username}")
        
        return cookies
    
    async def _get_proxy_config(self) -> Optional[Dict[str, Any]]:
        """Get smart proxy configuration"""
        try:
            # å°†APIé…ç½®å‚æ•°ä¼ é€’ç»™æ™ºèƒ½ä»£ç†ç®¡ç†å™¨ï¼ŒAPIå‚æ•°ä¼˜å…ˆçº§é«˜äºç¯å¢ƒå˜é‡
            return await self.smart_proxy_manager.get_proxy_config(
                override_use_proxy_pool=self.config.use_proxy_pool,
                override_proxy=self.config.proxy
            )
        except Exception as e:
            self.logger.error(f"è·å–ä»£ç†é…ç½®å¤±è´¥: {e}")
            return None
    
    def _extract_tweet_id(self, url: str) -> str:
        """Extract tweet ID from Twitter URL"""
        if '/status/' in url:
            return url.split('/status/')[-1].split('?')[0]
        return ""
    
    async def _make_view_request(self, url: str, account: Account, client: httpx.AsyncClient) -> bool:
        """Make a simple page request to increment tweet view count - simpler and more reliable"""
        try:
            tweet_id = self._extract_tweet_id(url)
            if not tweet_id:
                self.logger.error(f"Could not extract tweet ID from {url}")
                return False
            
            # ä½¿ç”¨ç¼“å­˜çš„sessionæˆ–é‡æ–°ç™»å½•
            cookies = await self._get_or_refresh_session(account, client)
            if not cookies:
                self.logger.warning(f"Failed to get session for account {account.username}, skipping")
                return False
            
            # ç›´æ¥è®¿é—®æ¨æ–‡é¡µé¢ - è¿™æ˜¯æœ€ç®€å•ä¸”æœ‰æ•ˆçš„æ–¹æ³•
            # æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è®¿é—®æ¨æ–‡é¡µé¢çš„è¡Œä¸º
            
            headers = {
                "User-Agent": random.choice(self.user_agents),
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.5",
                "Accept-Encoding": "gzip, deflate, br",
                "Referer": "https://twitter.com/",
                "DNT": "1",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate", 
                "Sec-Fetch-Site": "same-origin",
                "Cache-Control": "no-cache",
                "Pragma": "no-cache"
            }
            
            self.logger.info(f"Making page request for tweet {tweet_id} with account {account.username}")
            self.logger.debug(f"Target URL: {url}")
            
            # ç›´æ¥è®¿é—®æ¨æ–‡é¡µé¢
            response = await client.get(
                url,
                headers=headers,
                cookies=cookies
            )
            
            # Check if request was successful
            if response.status_code == 200:
                # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«æ¨æ–‡å†…å®¹
                page_content = response.text
                
                # éªŒè¯é¡µé¢ç¡®å®åŠ è½½äº†æ¨æ–‡å†…å®¹
                if any(indicator in page_content.lower() for indicator in ['tweet', 'status', tweet_id]):
                    self.stats["successful_requests"] += 1
                    self.stats["views_per_url"][url] = self.stats["views_per_url"].get(url, 0) + 1
                    self.logger.info(f"âœ… Page request successful for tweet {tweet_id} (Account: {account.username})")
                    self.logger.debug(f"Page content length: {len(page_content)} characters")
                    return True
                else:
                    self.logger.warning(f"âš ï¸ Page loaded but missing tweet content for {tweet_id}")
                    self.stats["failed_requests"] += 1
                    return False
                    
            else:
                self.stats["failed_requests"] += 1
                self.logger.error(f"âŒ Page request failed with status {response.status_code} for tweet {tweet_id} (Account: {account.username})")
                
                # Log more details for authentication errors
                if response.status_code == 401:
                    self.logger.error(f"Auth error - check auth_token: {cookies.get('auth_token', 'None')[:10]}...")
                elif response.status_code == 403:
                    self.logger.error(f"Forbidden - Account may be suspended or access restricted")
                elif response.status_code == 404:
                    self.logger.error(f"Tweet not found - URL may be invalid: {url}")
                
                self.logger.debug(f"Response content: {response.text[:500]}...")
                return False
                
        except Exception as e:
            self.stats["failed_requests"] += 1
            self.logger.error(f"âŒ Page request error for tweet {tweet_id} (Account: {account.username}): {str(e)}", exc_info=True)
            return False
        finally:
            self.stats["total_requests"] += 1
    
    async def _worker(self, worker_id: int):
        """Worker coroutine to process requests"""
        try:
            accounts = self.account_manager.get_all_accounts()
            if not accounts:
                self.logger.error("No accounts available")
                return
            
            self.logger.debug(f"Worker {worker_id} starting with {len(accounts)} accounts")
            
            # Get smart proxy configuration
            proxy_config = await self._get_proxy_config()
            
            if proxy_config:
                proxy_type = "æ™ºèƒ½ä»£ç†"
                if "socks5://" in str(proxy_config):
                    proxy_type = "ä»£ç†æ± "
                elif "127.0.0.1" in str(proxy_config):
                    proxy_type = "æœ¬åœ°ä»£ç†"
                self.logger.info(f"Worker {worker_id} ä½¿ç”¨{proxy_type}")
            else:
                self.logger.info(f"Worker {worker_id} ä½¿ç”¨ç›´è¿")
            
            # Create client with proper timeout and proxy settings
            client_config = {
                "timeout": httpx.Timeout(self.config.timeout),
                "follow_redirects": True,
                "verify": False,  # Disable SSL verification for proxies
                "http2": False,  # Disable HTTP/2 for better proxy compatibility
                "limits": httpx.Limits(max_keepalive_connections=5, max_connections=10),
                "trust_env": False  # Ignore system proxy environment variables
            }
            
            if proxy_config:
                client_config["proxies"] = proxy_config
                self.logger.info(f"Worker {worker_id} using proxy")
            else:
                self.logger.info(f"Worker {worker_id} no proxy configured")
            
            async with httpx.AsyncClient(**client_config) as client:
                while self.running and not self._stop_event.is_set():
                    # Check if target views reached
                    total_views = sum(self.stats["views_per_url"].values())
                    if total_views >= self.config.target_views:
                        self.logger.info(f"ğŸ¯ Target views reached: {total_views}/{self.config.target_views}")
                        self._stop_event.set()
                        break
                    
                    # Select random URL and account
                    url = random.choice(self.config.target_urls)
                    account = random.choice(accounts)
                    
                    # Make GraphQL request to increment view count
                    success = await self._make_view_request(url, account, client)
                    
                    # Retry logic
                    if not success and self.config.retry_on_failure:
                        for retry in range(self.config.max_retries):
                            await asyncio.sleep(0.5)
                            # Try with different proxy configuration
                            new_proxy_config = await self._get_proxy_config()
                            if new_proxy_config:
                                client._proxies = new_proxy_config
                            
                            success = await self._make_view_request(url, account, client)
                            if success:
                                break
                    
                    # Random delay between requests
                    delay = random.uniform(*self.config.request_interval)
                    await asyncio.sleep(delay)
        except Exception as e:
            self.logger.error(f"Worker {worker_id} crashed: {e}", exc_info=True)
    
    async def _test_proxy_connection(self) -> bool:
        """Test smart proxy connection before starting"""
        # æ³¨æ„ï¼štest_proxy_connection éœ€è¦å•ç‹¬çš„å®ç°ï¼Œå› ä¸ºå®ƒéœ€è¦å®é™…åˆ›å»ºHTTPå®¢æˆ·ç«¯
        # è¿™é‡Œæˆ‘ä»¬å…ˆç®€åŒ–å®ç°ï¼Œåç»­å¯ä»¥æ”¹è¿›
        try:
            proxy_config = await self._get_proxy_config()
            if proxy_config:
                self.logger.info("âœ… ä»£ç†é…ç½®è·å–æˆåŠŸï¼Œè¿æ¥æµ‹è¯•é€šè¿‡")
                return True
            else:
                self.logger.info("âœ… ç›´è¿æ¨¡å¼ï¼Œè¿æ¥æµ‹è¯•é€šè¿‡")
                return True
        except Exception as e:
            self.logger.error(f"âŒ ä»£ç†è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def start(self):
        """Start the fast view booster"""
        self.running = True
        self._stop_event.clear()
        self.stats["start_time"] = datetime.now()
        
        self.logger.info(f"ğŸš€ Starting Fast View Booster")
        self.logger.info(f"ğŸ“Š Target views: {self.config.target_views}")
        self.logger.info(f"ğŸ”— URLs: {self.config.target_urls}")
        
        accounts = self.account_manager.get_all_accounts()
        self.logger.info(f"ğŸ‘¥ Accounts: {len(accounts)}")
        
        # æ˜¾ç¤ºæ™ºèƒ½ä»£ç†ç®¡ç†å™¨çŠ¶æ€
        proxy_status = self.smart_proxy_manager.get_status()
        self.logger.info(f"ğŸŒ ä»£ç†æ¨¡å¼: {proxy_status['network_mode']}")
        if proxy_status['proxy_pool_count'] > 0:
            self.logger.info(f"ğŸŒ ä»£ç†æ± : {proxy_status['proxy_pool_count']} ä¸ªä»£ç†")
        
        # Test proxy connection first (non-blocking)
        if not await self._test_proxy_connection():
            self.logger.warning("âš ï¸ Proxy connection test failed, but continuing with execution...")
        
        # Create worker tasks
        workers = []
        self.logger.info(f"Creating {self.config.max_concurrent_requests} workers...")
        for i in range(self.config.max_concurrent_requests):
            self.logger.debug(f"Creating worker {i}")
            worker = asyncio.create_task(self._worker(i))
            workers.append(worker)
        
        self.logger.info(f"All workers created, waiting for completion...")
        
        # Wait for completion or stop signal
        try:
            await self._stop_event.wait()
        finally:
            self.running = False
            # Cancel all workers
            for worker in workers:
                worker.cancel()
            
            # Wait for workers to finish
            await asyncio.gather(*workers, return_exceptions=True)
            
            # Print final statistics
            self._print_stats()
    
    def stop(self):
        """Stop the fast view booster"""
        self.logger.info("â¹ï¸ Stopping Fast View Booster")
        self.running = False
        self._stop_event.set()
    
    def _print_stats(self):
        """Print statistics"""
        duration = (datetime.now() - self.stats["start_time"]).total_seconds()
        total_views = sum(self.stats["views_per_url"].values())
        
        self.logger.info("=" * 50)
        self.logger.info("ğŸ“ˆ Fast View Booster Statistics")
        self.logger.info(f"â±ï¸ Duration: {duration:.1f} seconds")
        self.logger.info(f"ğŸ“Š Total Views: {total_views}")
        self.logger.info(f"âœ… Successful Requests: {self.stats['successful_requests']}")
        self.logger.info(f"âŒ Failed Requests: {self.stats['failed_requests']}")
        self.logger.info(f"âš¡ Requests/sec: {self.stats['total_requests']/duration:.2f}")
        
        if total_views > 0:
            self.logger.info(f"ğŸ¯ Views per URL:")
            for url, views in self.stats["views_per_url"].items():
                self.logger.info(f"   {url}: {views} views")
    
    def get_stats(self) -> Dict[str, Any]:
        """Get current statistics"""
        duration = (datetime.now() - self.stats["start_time"]).total_seconds()
        total_views = sum(self.stats["views_per_url"].values())
        
        return {
            "status": "running" if self.running else "stopped",
            "duration_seconds": duration,
            "total_views": total_views,
            "target_views": self.config.target_views,
            "progress_percentage": (total_views / self.config.target_views * 100) if self.config.target_views > 0 else 0,
            "successful_requests": self.stats["successful_requests"],
            "failed_requests": self.stats["failed_requests"],
            "total_requests": self.stats["total_requests"],
            "requests_per_second": self.stats["total_requests"] / duration if duration > 0 else 0,
            "views_per_url": self.stats["views_per_url"],
            "start_time": self.stats["start_time"].isoformat()
        }


# Export classes
__all__ = ['FastViewBooster', 'FastBoosterConfig']