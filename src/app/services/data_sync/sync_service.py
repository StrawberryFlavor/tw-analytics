"""
æ•°æ®åŒæ­¥æœåŠ¡

è´Ÿè´£ä» campaign_task_submission åŒæ­¥æ•°æ®åˆ° campaign_tweet_snapshot
"""

import asyncio
import inspect
import logging
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import aiomysql

from ..database import DatabaseService, CampaignTweetSnapshot
from .sync_models import TaskSubmission, SyncRecord, SyncOperation
from .error_handler import error_handler, ErrorCategory, ErrorAction
from ...core.config_factory import get_sync_config, SyncConfig
from ..data_sources.extractors.rate_limit_detector import rate_limit_detector


@dataclass
class SyncResult:
    """åŒæ­¥ç»“æœ"""
    total_processed: int = 0
    created_count: int = 0
    updated_count: int = 0
    skipped_count: int = 0
    error_count: int = 0
    processing_time: float = 0.0
    errors: List[str] = None
    
    def __post_init__(self):
        if self.errors is None:
            self.errors = []
    
    @property
    def success_rate(self) -> float:
        """æˆåŠŸç‡"""
        if self.total_processed == 0:
            return 0.0
        return ((self.created_count + self.updated_count + self.skipped_count) / self.total_processed) * 100


class CampaignDataSyncService:
    """æ•°æ®åŒæ­¥æœåŠ¡ - å•ä¸€èŒè´£åŸåˆ™"""
    
    def __init__(self, 
                 database_service: DatabaseService,
                 config: SyncConfig = None):
        """
        åˆå§‹åŒ–åŒæ­¥æœåŠ¡
        
        Args:
            database_service: æ•°æ®åº“æœåŠ¡
            config: åŒæ­¥é…ç½®
        """
        self.db_service = database_service
        self.config = config or get_sync_config()
        self.logger = logging.getLogger(__name__)
        self._twitter_service = None  # ç¼“å­˜TwitteræœåŠ¡å®ä¾‹
        
        self.logger.info(f"ğŸ”„ æ•°æ®åŒæ­¥æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ğŸ“‹ é…ç½®: æ‰¹æ¬¡å¤§å°={self.config.sync_batch_size}, å¹¶å‘={self.config.max_concurrent_syncs}")
    
    async def _get_database_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥ - ç»Ÿä¸€é…ç½®ç®¡ç†"""
        from ...core.config_factory import get_db_config
        
        db_config = get_db_config(production=True)
        connection_params = db_config.get_connection_params()
        
        return await aiomysql.connect(**connection_params)
    
    async def sync_all_data(self) -> SyncResult:
        """
        åŒæ­¥æ‰€æœ‰æ•°æ®
        
        Returns:
            åŒæ­¥ç»“æœ
        """
        start_time = time.time()
        result = SyncResult()
        
        try:
            self.logger.info("ğŸ”„ å¼€å§‹æ•°æ®åŒæ­¥...")
            
            # 1. åˆ†æéœ€è¦åŒæ­¥çš„æ•°æ®
            sync_records = await self._analyze_sync_needs()
            
            if not sync_records:
                self.logger.info("âœ… æ²¡æœ‰éœ€è¦åŒæ­¥çš„æ•°æ®")
                return result
            
            result.total_processed = len(sync_records)
            self.logger.info(f"ğŸ“Š æ‰¾åˆ° {len(sync_records)} æ¡è®°å½•éœ€è¦åŒæ­¥")
            
            # æŒ‰æ“ä½œç±»å‹ç»Ÿè®¡
            create_count = sum(1 for r in sync_records if r.operation == SyncOperation.CREATE)
            update_count = sum(1 for r in sync_records if r.operation == SyncOperation.UPDATE)
            
            self.logger.info(f"   éœ€è¦åˆ›å»º: {create_count} æ¡")
            self.logger.info(f"   éœ€è¦æ›´æ–°: {update_count} æ¡")
            
            if self.config.dry_run:
                self.logger.info("ğŸ§ª æ¼”ç»ƒæ¨¡å¼ï¼Œä¸ä¼šå®é™…ä¿®æ”¹æ•°æ®")
                result.created_count = create_count
                result.updated_count = update_count
                return result
            
            # 2. åˆ†æ‰¹å¤„ç†åŒæ­¥
            batches = self._create_batches(sync_records)
            
            for i, batch in enumerate(batches, 1):
                self.logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {i}/{len(batches)}: {len(batch)} æ¡è®°å½•")
                
                batch_result = await self._process_batch(batch)
                
                # åˆå¹¶ç»“æœ
                result.created_count += batch_result.created_count
                result.updated_count += batch_result.updated_count
                result.skipped_count += batch_result.skipped_count
                result.error_count += batch_result.error_count
                result.errors.extend(batch_result.errors)
                
                # æ‰¹æ¬¡é—´å»¶è¿Ÿ
                if i < len(batches):
                    await asyncio.sleep(self.config.sync_retry_delay)
            
            result.processing_time = time.time() - start_time
            
            self.logger.info("âœ… æ•°æ®åŒæ­¥å®Œæˆ!")
            self.logger.info(f"ğŸ“Š ç»“æœ: åˆ›å»º={result.created_count}, æ›´æ–°={result.updated_count}, "
                           f"è·³è¿‡={result.skipped_count}, é”™è¯¯={result.error_count}")
            self.logger.info(f"â±ï¸  ç”¨æ—¶: {result.processing_time:.1f}ç§’")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®åŒæ­¥å¤±è´¥: {e}")
            result.processing_time = time.time() - start_time
            result.errors.append(f"åŒæ­¥è¿‡ç¨‹å¼‚å¸¸: {str(e)}")
            raise
    
    async def _analyze_sync_needs(self) -> List[SyncRecord]:
        """åˆ†æåŒæ­¥éœ€æ±‚"""
        self.logger.info(f"ğŸ” åˆ†æåŒæ­¥éœ€æ±‚ (æ¨¡å¼: {self.config.sync_mode})...")
        
        if self.config.sync_mode == "update_all":
            return await self._analyze_update_all_needs()
        elif self.config.sync_mode == "priority_new":
            return await self._analyze_priority_new_needs()
        else:
            return await self._analyze_missing_only_needs()
    
    async def _analyze_missing_only_needs(self) -> List[SyncRecord]:
        """åˆ†æç¼ºå¤±è®°å½•çš„åŒæ­¥éœ€æ±‚"""
        # è·å–æ‰€æœ‰æœ‰æ•ˆçš„æäº¤è®°å½•
        # æ³¨æ„ï¼šéœ€è¦ä»x_linked_to URLä¸­æå–çº¯tweet IDæ¥ä¸campaign_tweet_snapshot.tweet_idåŒ¹é…
        query = """
        SELECT DISTINCT
            cts.x_linked_to as target_tweet_url,
            SUBSTRING_INDEX(SUBSTRING_INDEX(cts.x_linked_to, '/status/', -1), '?', 1) as target_tweet_id,
            cts.id,
            cts.task_id,
            cts.submitter_uid,
            cts.x_type,
            cts.x_tweet_id,
            cts.is_valid,
            cts.view_count,
            cts.reward_amount,
            cts.status,
            cts.created_at,
            cts.is_del,
            cts.updated_at,
            cts.yaps,
            cst.tweet_id as existing_tweet_id,
            cst.views as existing_views
        FROM campaign_task_submission cts
        LEFT JOIN campaign_tweet_snapshot cst ON SUBSTRING_INDEX(SUBSTRING_INDEX(cts.x_linked_to, '/status/', -1), '?', 1) = cst.tweet_id
        WHERE cts.x_linked_to IS NOT NULL 
        AND cts.x_linked_to != ''
        AND cts.is_del = 0
        ORDER BY cts.x_linked_to, cts.created_at DESC
        """
        
        # ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®åº“è¿æ¥æ–¹æ³•
        connection = await self._get_database_connection()
        cursor = await connection.cursor(aiomysql.DictCursor)
        await cursor.execute(query)
        records = await cursor.fetchall()
        await cursor.close()
        connection.close()
        
        # åˆ†ææ¯æ¡è®°å½•çš„åŒæ­¥éœ€æ±‚
        sync_records = []
        processed_tweets = set()  # é¿å…é‡å¤å¤„ç†åŒä¸€ä¸ªæ¨æ–‡ID
        
        for record in records:
            target_tweet_id = record['target_tweet_id']  # è¿™æ˜¯ä»URLæå–çš„çº¯tweet ID
            target_tweet_url = record['target_tweet_url']  # è¿™æ˜¯å®Œæ•´çš„URL
            
            # è·³è¿‡å·²å¤„ç†çš„ç›®æ ‡æ¨æ–‡IDï¼ˆæ¯ä¸ªç›®æ ‡æ¨æ–‡IDåªå¤„ç†ä¸€æ¬¡ï¼‰
            if target_tweet_id in processed_tweets:
                continue
            
            processed_tweets.add(target_tweet_id)
            
            # åˆ›å»ºTaskSubmissionå¯¹è±¡
            submission = TaskSubmission(
                id=record['id'],
                task_id=record['task_id'],
                submitter_uid=record['submitter_uid'],
                x_tweet_id=record['x_tweet_id'],  # ç”¨æˆ·è‡ªå·±çš„æ¨æ–‡ID
                x_type=record['x_type'],
                x_linked_to=target_tweet_url,  # ç›®æ ‡æ¨æ–‡å®Œæ•´URL
                is_valid=record['is_valid'],
                view_count=record['view_count'],
                reward_amount=float(record['reward_amount']) if record['reward_amount'] else None,
                status=record['status'],
                created_at=record['created_at'],
                is_del=record['is_del'],
                updated_at=record['updated_at'],
                yaps=record['yaps']
            )
            
            # åˆ¤æ–­åŒæ­¥æ“ä½œç±»å‹ - åŸºäºç›®æ ‡æ¨æ–‡ID
            if not record['existing_tweet_id']:
                # éœ€è¦åˆ›å»ºæ–°è®°å½• - ä¸ºç›®æ ‡æ¨æ–‡åˆ›å»ºsnapshot
                sync_record = SyncRecord(
                    tweet_id=target_tweet_id,  # ä½¿ç”¨ç›®æ ‡æ¨æ–‡ID
                    operation=SyncOperation.CREATE,
                    submission_data=submission,
                    reason="ç¼ºå¤±ç›®æ ‡æ¨æ–‡å¿«ç…§è®°å½•"
                )
                sync_records.append(sync_record)
                
            elif record['existing_views'] != record['view_count']:
                # éœ€è¦æ›´æ–°ç°æœ‰è®°å½•
                sync_record = SyncRecord(
                    tweet_id=target_tweet_id,  # ä½¿ç”¨ç›®æ ‡æ¨æ–‡ID
                    operation=SyncOperation.UPDATE,
                    submission_data=submission,
                    reason=f"viewsä¸ä¸€è‡´: {record['existing_views']} -> {record['view_count']}"
                )
                sync_records.append(sync_record)
            
            # å¦‚æœæ•°æ®ä¸€è‡´åˆ™è·³è¿‡
        
        self.logger.info(f"ğŸ” åˆ†æå®Œæˆ: éœ€è¦å¤„ç† {len(sync_records)} æ¡è®°å½•")
        return sync_records
    
    async def _analyze_update_all_needs(self) -> List[SyncRecord]:
        """åˆ†æå…¨éƒ¨æ›´æ–°éœ€æ±‚ - æ›´æ–°æ‰€æœ‰ç°æœ‰è®°å½•çš„Twitteræ•°æ®"""
        # è·å–æ‰€æœ‰campaign_tweet_snapshotä¸­çš„è®°å½•
        query = """
        SELECT 
            tweet_id,
            author_username,
            views,
            created_at
        FROM campaign_tweet_snapshot
        WHERE tweet_id IS NOT NULL 
        AND tweet_id != ''
        ORDER BY id DESC
        """
        
        # ä½¿ç”¨åŸå§‹æŸ¥è¯¢
        connection = await self._get_database_connection()
        
        cursor = await connection.cursor(aiomysql.DictCursor)
        await cursor.execute(query)
        records = await cursor.fetchall()
        await cursor.close()
        connection.close()
        
        # ä¸ºæ¯æ¡ç°æœ‰è®°å½•åˆ›å»ºæ›´æ–°ä»»åŠ¡
        sync_records = []
        for record in records:
            # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„TaskSubmissionæ¥å…¼å®¹ç°æœ‰é€»è¾‘
            submission = TaskSubmission(
                id=0,  # è™šæ‹ŸID
                task_id=0,
                submitter_uid=0,
                x_tweet_id=record['tweet_id'],
                x_type='refresh',  # æ ‡è®°ä¸ºåˆ·æ–°æ“ä½œ
                x_linked_to=None,
                is_valid=1,
                view_count=None,  # ä¸ä½¿ç”¨æ—§çš„view_countï¼Œå®Œå…¨ä»Twitterè·å–
                reward_amount=None,
                status='refresh',
                created_at=record['created_at'],
                is_del=0,
                updated_at=record['created_at'],  # ä½¿ç”¨created_atæ›¿ä»£
                yaps=None
            )
            
            sync_record = SyncRecord(
                tweet_id=record['tweet_id'],
                operation=SyncOperation.UPDATE,
                submission_data=submission,
                reason=f"å…¨éƒ¨æ›´æ–°æ¨¡å¼ - åˆ·æ–°Twitteræ•°æ®"
            )
            sync_records.append(sync_record)
        
        self.logger.info(f"ğŸ”„ å…¨éƒ¨æ›´æ–°æ¨¡å¼: æ‰¾åˆ° {len(sync_records)} æ¡ç°æœ‰è®°å½•éœ€è¦åˆ·æ–°")
        return sync_records
    
    async def _analyze_priority_new_needs(self) -> List[SyncRecord]:
        """åˆ†æä¼˜å…ˆçº§åŒæ­¥éœ€æ±‚ - ä¸“é—¨å¤„ç†ä»æœªåŒæ­¥è¿‡çš„æ•°æ®"""
        # æŸ¥æ‰¾åœ¨campaign_task_submissionä¸­ä½†ä¸åœ¨campaign_tweet_snapshotä¸­çš„è®°å½•
        query = """
        SELECT DISTINCT
            cts.x_linked_to as target_tweet_url,
            SUBSTRING_INDEX(SUBSTRING_INDEX(cts.x_linked_to, '/status/', -1), '?', 1) as target_tweet_id,
            cts.id,
            cts.task_id,
            cts.submitter_uid,
            cts.x_type,
            cts.x_tweet_id,
            cts.is_valid,
            cts.view_count,
            cts.reward_amount,
            cts.status,
            cts.created_at,
            cts.is_del,
            cts.updated_at,
            cts.yaps
        FROM campaign_task_submission cts
        LEFT JOIN campaign_tweet_snapshot cst ON SUBSTRING_INDEX(SUBSTRING_INDEX(cts.x_linked_to, '/status/', -1), '?', 1) = cst.tweet_id
        WHERE cts.x_linked_to IS NOT NULL 
        AND cts.x_linked_to != ''
        AND cts.is_del = 0
        AND cts.is_valid = 1
        AND cst.tweet_id IS NULL  -- å…³é”®æ¡ä»¶ï¼šåœ¨campaign_tweet_snapshotä¸­ä¸å­˜åœ¨
        ORDER BY cts.created_at ASC  -- æŒ‰æ—¶é—´å‡åºï¼Œä¼˜å…ˆå¤„ç†æ—©æœŸæ•°æ®
        """
        
        # ä½¿ç”¨åŸå§‹æŸ¥è¯¢
        connection = await self._get_database_connection()
        
        cursor = await connection.cursor(aiomysql.DictCursor)
        await cursor.execute(query)
        records = await cursor.fetchall()
        await cursor.close()
        connection.close()
        
        # åˆ†ææ¯æ¡è®°å½•çš„åŒæ­¥éœ€æ±‚
        sync_records = []
        processed_tweets = set()  # é¿å…é‡å¤å¤„ç†åŒä¸€ä¸ªæ¨æ–‡ID
        
        for record in records:
            target_tweet_id = record['target_tweet_id']  # è¿™æ˜¯ä»URLæå–çš„çº¯tweet ID
            target_tweet_url = record['target_tweet_url']  # è¿™æ˜¯å®Œæ•´çš„URL
            
            # è·³è¿‡å·²å¤„ç†çš„ç›®æ ‡æ¨æ–‡IDï¼ˆæ¯ä¸ªç›®æ ‡æ¨æ–‡IDåªå¤„ç†ä¸€æ¬¡ï¼‰
            if target_tweet_id in processed_tweets:
                continue
            
            processed_tweets.add(target_tweet_id)
            
            # åˆ›å»ºTaskSubmissionå¯¹è±¡
            submission = TaskSubmission(
                id=record['id'],
                task_id=record['task_id'],
                submitter_uid=record['submitter_uid'],
                x_tweet_id=record['x_tweet_id'],  # ç”¨æˆ·è‡ªå·±çš„æ¨æ–‡ID
                x_type=record['x_type'],
                x_linked_to=target_tweet_url,  # ç›®æ ‡æ¨æ–‡å®Œæ•´URL
                is_valid=record['is_valid'],
                view_count=record['view_count'],
                reward_amount=float(record['reward_amount']) if record['reward_amount'] else None,
                status=record['status'],
                created_at=record['created_at'],
                is_del=record['is_del'],
                updated_at=record['updated_at'],
                yaps=record['yaps']
            )
            
            # æ‰€æœ‰ä¼˜å…ˆçº§è®°å½•éƒ½æ˜¯éœ€è¦æ–°åˆ›å»ºçš„
            sync_record = SyncRecord(
                tweet_id=target_tweet_id,  # ä½¿ç”¨ç›®æ ‡æ¨æ–‡ID
                operation=SyncOperation.CREATE,
                submission_data=submission,
                reason="ä¼˜å…ˆçº§åŒæ­¥ - ä»æœªåŒæ­¥è¿‡çš„ç›®æ ‡æ¨æ–‡"
            )
            sync_records.append(sync_record)
        
        self.logger.info(f"âš¡ ä¼˜å…ˆçº§åŒæ­¥: æ‰¾åˆ° {len(sync_records)} æ¡ä»æœªåŒæ­¥è¿‡çš„è®°å½•")
        return sync_records
    
    def _create_batches(self, sync_records: List[SyncRecord]) -> List[List[SyncRecord]]:
        """åˆ›å»ºæ‰¹æ¬¡"""
        batches = []
        for i in range(0, len(sync_records), self.config.sync_batch_size):
            batch = sync_records[i:i + self.config.sync_batch_size]
            batches.append(batch)
        return batches
    
    async def _process_batch(self, batch: List[SyncRecord]) -> SyncResult:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        result = SyncResult()
        
        for sync_record in batch:
            try:
                if sync_record.operation == SyncOperation.CREATE:
                    create_result = await self._create_snapshot_record(sync_record.submission_data)
                    if create_result == "success":
                        result.created_count += 1
                        self.logger.debug(f"âœ… åˆ›å»ºè®°å½•: {sync_record.tweet_id}")
                    elif create_result == "skipped":
                        result.skipped_count += 1
                        self.logger.debug(f"â­ï¸  è·³è¿‡è®°å½•: {sync_record.tweet_id} (æ¨æ–‡ä¸å­˜åœ¨)")
                    else:
                        result.error_count += 1
                        result.errors.append(f"åˆ›å»ºå¤±è´¥: {sync_record.tweet_id}")
                
                elif sync_record.operation == SyncOperation.UPDATE:
                    update_result = await self._update_snapshot_record(sync_record.submission_data)
                    if update_result == "success":
                        result.updated_count += 1
                        self.logger.debug(f"âœ… æ›´æ–°è®°å½•: {sync_record.tweet_id}")
                    elif update_result == "skipped":
                        result.skipped_count += 1
                        self.logger.debug(f"â­ï¸  è·³è¿‡è®°å½•: {sync_record.tweet_id} (æ¨æ–‡ä¸å­˜åœ¨)")
                    else:
                        result.error_count += 1
                        result.errors.append(f"æ›´æ–°å¤±è´¥: {sync_record.tweet_id}")
                
            except Exception as e:
                result.error_count += 1
                error_msg = f"å¤„ç† {sync_record.tweet_id} å¤±è´¥: {str(e)}"
                result.errors.append(error_msg)
                self.logger.error(f"âŒ {error_msg}")
        
        return result
    
    async def _create_snapshot_record(self, submission: TaskSubmission) -> str:
        """
        åˆ›å»ºå¿«ç…§è®°å½• - æ™ºèƒ½é”™è¯¯å¤„ç†ç¡®ä¿æŠ€æœ¯é”™è¯¯ä¸å½±å“å¸–å­çŠ¶æ€
        
        Args:
            submission: ä»»åŠ¡æäº¤è®°å½•
            
        Returns:
            str: å¤„ç†ç»“æœ
                - 'success': æˆåŠŸåˆ›å»º
                - 'skipped': è·³è¿‡å¤„ç†ï¼ˆå¯èƒ½æ˜¯æŠ€æœ¯é—®é¢˜æˆ–å†…å®¹é—®é¢˜ï¼‰
                - 'failed': å¤„ç†å¤±è´¥
                
        å¤„ç†é€»è¾‘:
            1. æŠ€æœ¯é”™è¯¯ï¼ˆç½‘ç»œã€æœåŠ¡å™¨ã€æµè§ˆå™¨ç­‰ï¼‰-> è·³è¿‡ä½†ä¸æ ‡è®°å¸–å­æ— æ•ˆ
            2. å†…å®¹é”™è¯¯ï¼ˆæ¨æ–‡åˆ é™¤ã€ç§å¯†ç­‰ï¼‰-> è·³è¿‡å¹¶æ ‡è®°å¸–å­æ— æ•ˆ
            3. é£æ§é”™è¯¯ -> ç­‰å¾…é‡è¯•
        """
        try:
            # å¿…é¡»å…ˆè·å–å®Œæ•´çš„Twitteræ•°æ® - ä½¿ç”¨ç›®æ ‡æ¨æ–‡ID(x_linked_to)
            twitter_data = None
            if self.config.enable_twitter_api:
                try:
                    twitter_data = await self._get_comprehensive_twitter_data(submission.x_linked_to)
                except Exception as e:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯é£æ§å¯¼è‡´çš„å¼‚å¸¸
                    if (hasattr(e, 'wait_time') and 
                        type(e).__name__ == 'RateLimitDetectedError'):
                        self.logger.warning(f"ğŸš¨ åˆ›å»ºè®°å½•æ—¶æ£€æµ‹åˆ°é£æ§ï¼Œå·²åœ¨è·å–æ•°æ®æ—¶å¤„ç†ç­‰å¾…: {submission.x_linked_to}")
                        # é£æ§å·²ç»åœ¨_get_comprehensive_twitter_dataä¸­å¤„ç†äº†ï¼Œè¿™é‡Œå†è¯•ä¸€æ¬¡
                        try:
                            twitter_data = await self._get_comprehensive_twitter_data(submission.x_linked_to)
                        except Exception as retry_e:
                            self.logger.error(f"âŒ é£æ§å¤„ç†åé‡è¯•ä»å¤±è´¥ {submission.x_linked_to}: {retry_e}")
                            return "failed"
                    else:
                        self.logger.error(f"âŒ è·å–Twitteræ•°æ®å¤±è´¥ {submission.x_linked_to}: {e}")
                        return "failed"
            
            # === æ™ºèƒ½é”™è¯¯å¤„ç†ï¼šåŒºåˆ†æŠ€æœ¯é”™è¯¯å’Œå†…å®¹é”™è¯¯ ===
            if not twitter_data:
                # æ•°æ®ä¸ºç©ºå¯èƒ½çš„åŸå› ï¼š
                # 1. ç½‘ç»œé—®é¢˜ã€æœåŠ¡å™¨é”™è¯¯ -> æŠ€æœ¯é”™è¯¯ï¼Œä¸å½±å“å¸–å­çŠ¶æ€
                # 2. æ¨æ–‡çœŸçš„è¢«åˆ é™¤ -> å†…å®¹é”™è¯¯ï¼Œéœ€è¦æ ‡è®°æ— æ•ˆ
                analysis = error_handler.analyze_error(
                    Exception("æ•°æ®æå–ä¸ºç©º"), 
                    "", 
                    "data_empty"
                )
                error_handler.log_error_analysis(analysis, submission.x_linked_to)
                
                # åªæœ‰ç¡®è®¤æ˜¯å†…å®¹é—®é¢˜æ‰æ ‡è®°å¸–å­æ— æ•ˆ
                if error_handler.should_mark_submission_invalid(analysis):
                    await self._mark_submission_invalid(submission.x_linked_to)
                
                return error_handler.get_return_status(analysis)
            
            # æ£€æŸ¥æ˜¯å¦ç¡®è®¤æ¨æ–‡å·²åˆ é™¤
            if isinstance(twitter_data, dict) and twitter_data.get('tweet_deleted'):
                analysis = error_handler.analyze_error(
                    Exception("æ¨æ–‡ä¸å­˜åœ¨"), 
                    f"æ¨æ–‡çŠ¶æ€: {twitter_data.get('reason')}", 
                    twitter_data.get('reason', 'tweet_deleted')
                )
                error_handler.log_error_analysis(analysis, submission.x_linked_to)
                
                if error_handler.should_mark_submission_invalid(analysis):
                    await self._mark_submission_invalid(submission.x_linked_to)
                
                return error_handler.get_return_status(analysis)
            
            # ä½¿ç”¨å®Œæ•´çš„Twitteræ•°æ®åˆ›å»ºè®°å½•
            # ä»URLä¸­æå–çº¯tweet IDç”¨äºæ•°æ®åº“å­˜å‚¨
            pure_tweet_id = self._extract_tweet_id_from_url(submission.x_linked_to)
            
            snapshot = CampaignTweetSnapshot(
                tweet_id=pure_tweet_id,  # ä½¿ç”¨æå–çš„çº¯tweet ID
                tweet_type=submission.x_type,
                success=True,
                message="ä»campaign_task_submissionåŒæ­¥åˆ›å»ºï¼ˆå«å®Œæ•´Twitteræ•°æ®ï¼‰",
                
                # Twitter APIè·å–çš„å®Œæ•´æ•°æ®
                author_username=twitter_data.get('author_username', 'unknown'),
                author_name=twitter_data.get('author_name'),
                author_avatar=twitter_data.get('author_avatar'),
                author_verified=twitter_data.get('author_verified', False),
                
                tweet_text=twitter_data.get('tweet_text'),
                tweet_time_utc=twitter_data.get('tweet_time_utc'),
                
                # æ¨æ–‡ç»Ÿè®¡æ•°æ® - ä¼˜å…ˆä½¿ç”¨submissionçš„view_count
                views=submission.view_count if submission.view_count else twitter_data.get('views'),
                replies=twitter_data.get('replies'),
                retweets=twitter_data.get('retweets'),  
                likes=twitter_data.get('likes'),
                quotes=twitter_data.get('quotes'),
                
                # æ±‡æ€»ä¿¡æ¯
                summary_total_tweets=twitter_data.get('summary_total_tweets'),
                summary_has_thread=twitter_data.get('summary_has_thread'),
                summary_has_replies=twitter_data.get('summary_has_replies'),
                
                # JSONç»“æ„æ•°æ®
                primary_tweet=twitter_data.get('primary_tweet'),
                thread=twitter_data.get('thread'),
                related=twitter_data.get('related')
            )
            
            # æ’å…¥æ•°æ®åº“
            success = await self.db_service.create_record(snapshot)
            if success:
                self.logger.debug(f"âœ… æˆåŠŸåˆ›å»ºå®Œæ•´è®°å½•: {submission.x_linked_to}, author: {twitter_data.get('author_username')}, views: {snapshot.views}")
                return "success"
            else:
                return "failed"
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºå¿«ç…§è®°å½•å¤±è´¥ {submission.x_linked_to}: {e}")
            return "failed"
    
    async def _update_snapshot_record(self, submission: TaskSubmission) -> str:
        """
        æ›´æ–°å¿«ç…§è®°å½• - æ™ºèƒ½é”™è¯¯å¤„ç†ç¡®ä¿æŠ€æœ¯é”™è¯¯ä¸å½±å“å¸–å­çŠ¶æ€
        
        Args:
            submission: ä»»åŠ¡æäº¤è®°å½•
            
        Returns:
            str: å¤„ç†ç»“æœ
                - 'success': æˆåŠŸæ›´æ–°
                - 'skipped': è·³è¿‡å¤„ç†ï¼ˆæŠ€æœ¯é—®é¢˜ä¿ç•™åŸçŠ¶æ€ï¼Œå†…å®¹é—®é¢˜æ ‡è®°æ— æ•ˆï¼‰
                - 'failed': å¤„ç†å¤±è´¥
                
        å¤„ç†é€»è¾‘:
            1. ä¼˜å…ˆä»Twitter APIè·å–æœ€æ–°æ•°æ®
            2. APIå¤±è´¥æ—¶fallbackåˆ°submissionæ•°æ®
            3. æŠ€æœ¯é”™è¯¯ä¸å½±å“å¸–å­çŠ¶æ€ï¼Œå†…å®¹é”™è¯¯æ ‡è®°æ— æ•ˆ
        """
        try:
            # è·å–ç°æœ‰è®°å½• - ä½¿ç”¨ä»URLæå–çš„çº¯tweet IDæŸ¥æ‰¾
            pure_tweet_id = self._extract_tweet_id_from_url(submission.x_linked_to)
            existing = await self.db_service.get_by_tweet_id(pure_tweet_id)
            if not existing:
                self.logger.warning(f"âš ï¸  æ‰¾ä¸åˆ°è¦æ›´æ–°çš„è®°å½•: {pure_tweet_id} (æ¥æºURL: {submission.x_linked_to})")
                return "failed"
            
            # å¦‚æœå¯ç”¨Twitter APIï¼Œè·å–æœ€æ–°æ•°æ® - ä½¿ç”¨ç›®æ ‡æ¨æ–‡ID(x_linked_to)
            if self.config.enable_twitter_api:
                try:
                    twitter_data = await self._get_comprehensive_twitter_data(submission.x_linked_to)
                    if twitter_data:
                        # æ›´æ–°æ‰€æœ‰å¯ç”¨å­—æ®µ
                        existing.author_username = twitter_data.get('author_username', existing.author_username)
                        existing.author_name = twitter_data.get('author_name') or existing.author_name
                        existing.author_avatar = twitter_data.get('author_avatar') or existing.author_avatar
                        existing.author_verified = twitter_data.get('author_verified', existing.author_verified)
                        
                        existing.tweet_text = twitter_data.get('tweet_text') or existing.tweet_text
                        existing.tweet_time_utc = twitter_data.get('tweet_time_utc') or existing.tweet_time_utc
                        
                        # æ›´æ–°ç»Ÿè®¡æ•°æ® - è¿™äº›æ˜¯æœ€é‡è¦çš„
                        existing.views = twitter_data.get('views') or existing.views
                        existing.replies = twitter_data.get('replies') or existing.replies
                        existing.retweets = twitter_data.get('retweets') or existing.retweets
                        existing.likes = twitter_data.get('likes') or existing.likes
                        existing.quotes = twitter_data.get('quotes') or existing.quotes
                        
                        # æ›´æ–°æ±‡æ€»ä¿¡æ¯
                        existing.summary_total_tweets = twitter_data.get('summary_total_tweets') or existing.summary_total_tweets
                        existing.summary_has_thread = twitter_data.get('summary_has_thread')
                        existing.summary_has_replies = twitter_data.get('summary_has_replies')
                        
                        # æ›´æ–°JSONæ•°æ®
                        existing.primary_tweet = twitter_data.get('primary_tweet') or existing.primary_tweet
                        existing.thread = twitter_data.get('thread') or existing.thread
                        existing.related = twitter_data.get('related') or existing.related
                        
                        existing.message = "æ•°æ®å·²ä»Twitter APIåˆ·æ–°"
                        self.logger.debug(f"âœ… ä»Twitteræ›´æ–°æ•°æ®: {submission.x_linked_to}, views: {existing.views}")
                    else:
                        # ä½¿ç”¨é”™è¯¯å¤„ç†å™¨åˆ†ææ•°æ®è·å–ç»“æœ
                        if isinstance(twitter_data, dict) and twitter_data.get('tweet_deleted'):
                            analysis = error_handler.analyze_error(
                                Exception("æ¨æ–‡ä¸å­˜åœ¨"), 
                                f"æ¨æ–‡çŠ¶æ€: {twitter_data.get('reason')}", 
                                twitter_data.get('reason', 'tweet_deleted')
                            )
                        else:
                            analysis = error_handler.analyze_error(
                                Exception("æ•°æ®æå–ä¸ºç©º"), 
                                "", 
                                "data_empty_update"
                            )
                        
                        error_handler.log_error_analysis(analysis, submission.x_linked_to)
                        
                        if error_handler.should_mark_submission_invalid(analysis):
                            await self._mark_submission_invalid(submission.x_linked_to)
                        
                        return error_handler.get_return_status(analysis)
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸  Twitter APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨submissionæ•°æ®: {submission.x_linked_to}: {e}")
                    # fallback åˆ° submission æ•°æ®
                    if submission.view_count is not None:
                        existing.views = submission.view_count
                    existing.tweet_type = submission.x_type
                    existing.message = "éƒ¨åˆ†æ•°æ®æ›´æ–°ï¼ˆTwitter APIå¼‚å¸¸ï¼‰"
            else:
                # ä¸ä½¿ç”¨Twitter APIæ—¶ï¼Œåªæ›´æ–°submissionæä¾›çš„æ•°æ®
                if submission.view_count is not None:
                    existing.views = submission.view_count
                existing.tweet_type = submission.x_type
                existing.message = "æ•°æ®æ›´æ–°ï¼ˆæœªä½¿ç”¨Twitter APIï¼‰"
            
            # æ³¨æ„ï¼šè¡¨ä¸­æ²¡æœ‰updated_atå­—æ®µï¼Œæ•°æ®åº“ä¼šè‡ªåŠ¨ç®¡ç†æ—¶é—´æˆ³
            
            # æ›´æ–°æ•°æ®åº“
            success = await self.db_service.update_record(existing)
            if success:
                return "success"
            else:
                return "failed"
            
        except Exception as e:
            self.logger.error(f"âŒ æ›´æ–°å¿«ç…§è®°å½•å¤±è´¥ {submission.x_linked_to}: {e}")
            return "failed"
    
    def _parse_twitter_timestamp(self, timestamp_str: str) -> Optional[datetime]:
        """è§£æTwitteræ—¶é—´æˆ³ä¸ºdatetimeå¯¹è±¡"""
        if not timestamp_str:
            return None
        
        try:
            from datetime import datetime
            # Twitter APIè¿”å›æ ¼å¼: '2025-08-07T22:48:55.000Z'
            if timestamp_str.endswith('Z'):
                # ç§»é™¤æ¯«ç§’å’ŒZï¼Œç„¶åè§£æ
                timestamp_str = timestamp_str.replace('.000Z', '').replace('Z', '')
                return datetime.fromisoformat(timestamp_str)
            elif 'T' in timestamp_str:
                return datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
            else:
                return datetime.fromisoformat(timestamp_str)
        except Exception as e:
            self.logger.warning(f"âš ï¸  æ—¶é—´æˆ³è§£æå¤±è´¥: {timestamp_str}, é”™è¯¯: {e}")
            return None

    def _extract_tweet_id_from_url(self, tweet_url: str) -> str:
        """ä»æ¨æ–‡URLä¸­æå–çº¯tweet ID"""
        try:
            # æ”¯æŒå¤šç§URLæ ¼å¼:
            # https://x.com/username/status/1234567890
            # https://twitter.com/username/status/1234567890
            import re
            pattern = r'/status/(\d+)'
            match = re.search(pattern, tweet_url)
            if match:
                return match.group(1)
            else:
                # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œè¿”å›åŸå§‹URLï¼ˆå‘åå…¼å®¹ï¼‰
                self.logger.warning(f"âš ï¸  æ— æ³•ä»URLæå–tweet ID: {tweet_url}")
                return tweet_url
        except Exception as e:
            self.logger.error(f"âŒ æå–tweet IDå¤±è´¥: {tweet_url}, é”™è¯¯: {e}")
            return tweet_url
    
    async def _get_comprehensive_twitter_data(self, tweet_url: str) -> Optional[Dict[str, Any]]:
        """è·å–å®Œæ•´æ¨æ–‡æ•°æ®"""
        try:
            # tweet_url å·²ç»æ˜¯å®Œæ•´çš„URLï¼Œä¸éœ€è¦æ‹¼æ¥
            # ä¾‹å¦‚: https://x.com/username/status/1234567890
            
            # è·å–TwitteræœåŠ¡å®ä¾‹
            twitter_service = await self._get_twitter_service()
            
            # è°ƒç”¨ç»¼åˆæ•°æ®æ¥å£è·å–å®Œæ•´ä¿¡æ¯
            comprehensive_data = await twitter_service.get_comprehensive_data(tweet_url)
            
            if not comprehensive_data:
                self.logger.warning(f"âš ï¸  Twitter APIè¿”å›å¤±è´¥: {tweet_url}")
                return None
            
            # å…¼å®¹æ–°çš„æ•°æ®ç»“æ„ï¼šç›´æ¥ä»æ ¹çº§åˆ«è·å–primary_tweet
            primary_tweet = comprehensive_data.get('primary_tweet', {})
            
            if not primary_tweet:
                # æ£€æŸ¥æ˜¯å¦æœ‰æå–å…ƒæ•°æ®ä¸­çš„é”™è¯¯ä¿¡æ¯
                extraction_metadata = comprehensive_data.get('extraction_metadata', {})
                error_msg = extraction_metadata.get('error', '')
                detailed_reason = extraction_metadata.get('detailed_reason', '')
                
                # ä½¿ç”¨é”™è¯¯å¤„ç†å™¨è¿›è¡Œæ™ºèƒ½åˆ†æ
                analysis = error_handler.analyze_error(
                    Exception(error_msg or "æ•°æ®æå–å¤±è´¥"),
                    error_msg,
                    detailed_reason,
                    extraction_metadata
                )
                
                error_handler.log_error_analysis(analysis, tweet_url)
                
                # æ ¹æ®åˆ†æç»“æœå†³å®šè¿”å›å€¼
                if analysis.category == ErrorCategory.CONTENT_ERROR:
                    # å†…å®¹é—®é¢˜ï¼šæ¨æ–‡ç¡®å®ä¸å­˜åœ¨
                    return {'tweet_deleted': True, 'reason': detailed_reason}
                elif analysis.category == ErrorCategory.RATE_LIMIT:
                    # é£æ§é—®é¢˜ï¼šæŠ›å‡ºç‰¹å®šå¼‚å¸¸
                    class RateLimitDetectedError(Exception):
                        def __init__(self, message: str, wait_time: int = 300):
                            super().__init__(message)
                            self.wait_time = wait_time
                    raise RateLimitDetectedError(f"æ£€æµ‹åˆ°Twitteré£æ§: {error_msg}", wait_time=analysis.wait_time or 300)
                else:
                    # æŠ€æœ¯é—®é¢˜ï¼šè¿”å›Noneï¼Œä¸å½±å“å¸–å­çŠ¶æ€
                    return None
            
            # è§£æå¹¶è¿”å›ç»“æ„åŒ–æ•°æ® - é€‚é…æ–°çš„TwitteræœåŠ¡æ•°æ®ç»“æ„
            author = primary_tweet.get('author', {})
            metrics = primary_tweet.get('metrics', {})
            
            return {
                # ä½œè€…ä¿¡æ¯
                'author_username': author.get('username', 'unknown'),
                'author_name': author.get('display_name'),
                'author_avatar': author.get('avatar_url'),
                'author_verified': bool(author.get('is_verified', False)),
                
                # æ¨æ–‡å†…å®¹
                'tweet_text': primary_tweet.get('text'),
                'tweet_time_utc': self._parse_twitter_timestamp(primary_tweet.get('timestamp')),
                
                # ç»Ÿè®¡æ•°æ®
                'views': metrics.get('views'),
                'replies': metrics.get('replies'),
                'retweets': metrics.get('retweets'),
                'likes': metrics.get('likes'),
                'quotes': metrics.get('quotes'),
                
                # æ±‡æ€»ä¿¡æ¯
                'summary_total_tweets': comprehensive_data.get('extraction_metadata', {}).get('total_tweets_found'),
                'summary_has_thread': bool(comprehensive_data.get('thread_tweets', [])),
                'summary_has_replies': bool(comprehensive_data.get('related_tweets', [])),
                
                # åŸå§‹JSONæ•°æ®
                'primary_tweet': primary_tweet,
                'thread': comprehensive_data.get('thread_tweets'),
                'related': comprehensive_data.get('related_tweets')
            }
            
        except Exception as e:
            # æ£€æŸ¥æ˜¯å¦æ˜¯é£æ§å¼‚å¸¸ï¼ˆæ£€æŸ¥å¼‚å¸¸åç§°å’Œå±æ€§ï¼‰
            if (hasattr(e, 'wait_time') and 
                type(e).__name__ == 'RateLimitDetectedError'):
                self.logger.warning(f"ğŸš¨ æ£€æµ‹åˆ°é£æ§ï¼Œç­‰å¾… {e.wait_time} ç§’åé‡è¯•: {tweet_url}")
                # ç­‰å¾…æŒ‡å®šæ—¶é—´
                await asyncio.sleep(e.wait_time)
                # é‡è¯•ä¸€æ¬¡
                try:
                    self.logger.info(f"ğŸ”„ é£æ§ç­‰å¾…å®Œæˆï¼Œé‡è¯•è·å–: {tweet_url}")
                    twitter_service = await self._get_twitter_service()
                    comprehensive_data = await twitter_service.get_comprehensive_data(tweet_url)
                    
                    if not comprehensive_data:
                        self.logger.warning(f"âš ï¸  é‡è¯•åä»æ— æ³•è·å–æ•°æ®: {tweet_url}")
                        return None
                        
                    # é‡æ–°è§£ææ•°æ®ï¼ˆé‡å¤ä¸Šé¢çš„é€»è¾‘ï¼‰
                    primary_tweet = comprehensive_data.get('primary_tweet', {})
                    if not primary_tweet:
                        # é‡è¯•åä¹Ÿæ£€æŸ¥é”™è¯¯ç±»å‹
                        extraction_metadata = comprehensive_data.get('extraction_metadata', {})
                        error_msg = extraction_metadata.get('error', '')
                        detailed_reason = extraction_metadata.get('detailed_reason', '')
                        
                        # æ ¹æ®è¯¦ç»†åŸå› åˆ†ç±»å¤„ç†
                        if detailed_reason in ['rate_limited', 'login_required', 'page_load_error', 'network_error']:
                            self.logger.error(f"âŒ æŠ€æœ¯é”™è¯¯ - é‡è¯•åä»ä¸º{detailed_reason}: {tweet_url}")
                            return None  # æŠ€æœ¯é—®é¢˜
                        elif detailed_reason in ['tweet_not_found', 'tweet_protected']:
                            self.logger.warning(f"âš ï¸  é‡è¯•ç¡®è®¤æ¨æ–‡çŠ¶æ€ - {detailed_reason}: {tweet_url}")
                        elif 'è¶…æ—¶' in error_msg or 'timeout' in error_msg.lower():
                            self.logger.error(f"âŒ æŠ€æœ¯é”™è¯¯ - é‡è¯•åè·å–æ¨æ–‡ä»è¶…æ—¶: {tweet_url} - {error_msg}")
                            return None  # è¶…æ—¶é—®é¢˜
                        elif 'å®ä¾‹' in error_msg or 'instance' in error_msg.lower():
                            self.logger.error(f"âŒ æŠ€æœ¯é”™è¯¯ - é‡è¯•åæµè§ˆå™¨å®ä¾‹ä»æœ‰é—®é¢˜: {tweet_url} - {error_msg}")
                            return None  # å®ä¾‹é—®é¢˜
                        else:
                            self.logger.warning(f"âš ï¸  é‡è¯•åæœªæ‰¾åˆ°ä¸»æ¨æ–‡æ•°æ®: {tweet_url} (åŸå› : {detailed_reason or error_msg})")
                        return None
                    
                    # è¿”å›è§£æçš„æ•°æ®
                    author = primary_tweet.get('author', {})
                    metrics = primary_tweet.get('metrics', {})
                    
                    return {
                        'author_username': author.get('username', 'unknown'),
                        'author_name': author.get('display_name'),
                        'author_avatar': author.get('avatar_url'),
                        'author_verified': bool(author.get('is_verified', False)),
                        'tweet_text': primary_tweet.get('text'),
                        'tweet_time_utc': self._parse_twitter_timestamp(primary_tweet.get('timestamp')),
                        'views': metrics.get('views'),
                        'replies': metrics.get('replies'),
                        'retweets': metrics.get('retweets'),
                        'likes': metrics.get('likes'),
                        'quotes': metrics.get('quotes'),
                        'summary_total_tweets': comprehensive_data.get('extraction_metadata', {}).get('total_tweets_found'),
                        'summary_has_thread': bool(comprehensive_data.get('thread_tweets', [])),
                        'summary_has_replies': bool(comprehensive_data.get('related_tweets', [])),
                        'primary_tweet': primary_tweet,
                        'thread': comprehensive_data.get('thread_tweets'),
                        'related': comprehensive_data.get('related_tweets')
                    }
                except Exception as retry_error:
                    self.logger.error(f"âŒ é£æ§ç­‰å¾…åé‡è¯•ä»å¤±è´¥ {tweet_url}: {retry_error}")
                    return None
            else:
                self.logger.error(f"âŒ è·å–æ¨æ–‡å®Œæ•´æ•°æ®å¤±è´¥ {tweet_url}: {e}")
                return None
    
    async def _mark_submission_invalid(self, target_tweet_id: str) -> bool:
        """å°†campaign_task_submissionä¸­çš„æ¨æ–‡è®°å½•æ ‡è®°ä¸ºæ— æ•ˆ - åŸºäºx_linked_toå­—æ®µ"""
        try:
            # ä½¿ç”¨åŸå§‹SQLæ›´æ–°is_validå­—æ®µ
            connection = await self._get_database_connection()
            
            cursor = await connection.cursor()
            
            # æ›´æ–°æ‰€æœ‰åŒ¹é…çš„è®°å½•ä¸ºæ— æ•ˆ - åŸºäºx_linked_toå­—æ®µ
            update_query = """
            UPDATE campaign_task_submission 
            SET is_valid = 0 
            WHERE x_linked_to = %s AND is_valid = 1
            """
            
            result = await cursor.execute(update_query, (target_tweet_id,))
            await connection.commit()
            
            if cursor.rowcount > 0:
                self.logger.info(f"âœ… æ ‡è®° {cursor.rowcount} æ¡submissionè®°å½•ä¸ºæ— æ•ˆ(åŸºäºx_linked_to): {target_tweet_id}")
                success = True
            else:
                self.logger.debug(f"ğŸ“ æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ›´æ–°çš„submissionè®°å½•(åŸºäºx_linked_to): {target_tweet_id}")
                success = True  # ä¸ç®—é”™è¯¯
            
            await cursor.close()
            connection.close()
            
            return success
            
        except Exception as e:
            self.logger.error(f"âŒ æ ‡è®°submissionä¸ºæ— æ•ˆå¤±è´¥(åŸºäºx_linked_to) {target_tweet_id}: {e}")
            return False
    
    async def _get_twitter_service(self):
        """è·å–TwitteræœåŠ¡å®ä¾‹ - å•ä¾‹æ¨¡å¼é¿å…é‡å¤åˆ›å»º"""
        if self._twitter_service is None:
            # ä½¿ç”¨ç°æœ‰çš„ç‹¬ç«‹TwitteræœåŠ¡åˆ›å»ºå‡½æ•°
            from ..data_updater.service import _create_standalone_twitter_service
            self._twitter_service = await _create_standalone_twitter_service()
        return self._twitter_service
    
    async def cleanup(self):
        """æ¸…ç†æœåŠ¡èµ„æº"""
        if self._twitter_service:
            try:
                # ä½¿ç”¨TwitteræœåŠ¡çš„cleanupæ–¹æ³•
                if hasattr(self._twitter_service, 'cleanup'):
                    await self._twitter_service.cleanup()
                else:
                    # å…¼å®¹æ—§ç‰ˆæœ¬ï¼šç›´æ¥æ¸…ç†æ•°æ®ç®¡ç†å™¨ä¸­çš„èµ„æº
                    if hasattr(self._twitter_service, 'data_manager'):
                        for source in self._twitter_service.data_manager.sources:
                            if hasattr(source, 'cleanup'):
                                if inspect.iscoroutinefunction(source.cleanup):
                                    await source.cleanup()
                                else:
                                    source.cleanup()
                self.logger.info("âœ… TwitteræœåŠ¡èµ„æºæ¸…ç†å®Œæˆ")
            except Exception as e:
                self.logger.error(f"âŒ TwitteræœåŠ¡æ¸…ç†å¤±è´¥: {e}")
            finally:
                self._twitter_service = None
    
    def get_sync_statistics(self) -> Dict[str, Any]:
        """è·å–åŒæ­¥ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'config': {
                'sync_batch_size': self.config.sync_batch_size,
                'max_concurrent_syncs': self.config.max_concurrent_syncs,
                'skip_invalid_records': self.config.skip_invalid_records,
                'mark_invalid_on_error': self.config.mark_invalid_on_error
            },
            'service_status': 'ready'
        }