"""
ä½¿ç”¨æµè§ˆå™¨æ± çš„å¢å¼ºPlaywrightæ•°æ®æº
æä¾›é«˜æ€§èƒ½å¹¶å‘æ•°æ®æå–èƒ½åŠ›
"""

import asyncio
import logging
import os
import time
from typing import List, Optional, Dict, Any, Set
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

from .base import BaseDataSource
from .extractors import TweetDataExtractor
from .extractors.rate_limit_detector import rate_limit_detector
from ...core.interfaces import TweetData, UserData
from ...core.exceptions import DataSourceError, NotFoundError
from ..browser_pool import BrowserPool


class PlaywrightPooledSource(BaseDataSource):
    """
    ä½¿ç”¨æµè§ˆå™¨æ± çš„å¢å¼ºPlaywrightæ•°æ®æº
    
    ç‰¹æ€§:
    - æµè§ˆå™¨å®ä¾‹æ± åŒ–ï¼Œé¿å…é‡å¤å¯åŠ¨å¼€é”€
    - å¹¶å‘è¯·æ±‚å¤„ç†
    - æ™ºèƒ½è¯·æ±‚å»é‡
    - é«˜æ•ˆçš„æ‰¹é‡å¤„ç†
    - è‡ªåŠ¨æ•…éšœæ¢å¤
    """
    
    def __init__(self, 
                 pool_min_size: int = 2,
                 pool_max_size: int = 6,
                 max_concurrent_requests: int = None):
        """
        åˆå§‹åŒ–æ± åŒ–æ•°æ®æº (ç®€åŒ–ç‰ˆé…ç½®)
        
        Args:
            pool_min_size: æµè§ˆå™¨æ± æœ€å°å¤§å°
            pool_max_size: æµè§ˆå™¨æ± æœ€å¤§å¤§å°  
            max_concurrent_requests: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°ï¼ˆé»˜è®¤ä¸ºæ± å¤§å°ï¼‰
        """
        super().__init__("PlaywrightPooled")
        
        # æµè§ˆå™¨æ± é…ç½®
        self.pool_min_size = pool_min_size
        self.pool_max_size = pool_max_size
        self.max_concurrent_requests = max_concurrent_requests or pool_max_size
        
        # æµè§ˆå™¨æ± å®ä¾‹
        self._browser_pool: Optional[BrowserPool] = None
        self._pool_initialized = False
        
        # å¹¶å‘æ§åˆ¶
        self._semaphore = asyncio.Semaphore(self.max_concurrent_requests)
        self._request_queue = asyncio.Queue()
        self._active_requests: Set[str] = set()  # é˜²é‡å¤è¯·æ±‚
        
        # æ€§èƒ½ç»Ÿè®¡
        self.last_request_time = 0
        self.request_interval = 1.0  # è¯·æ±‚é—´éš”ï¼ˆæ± åŒ–åå¯ä»¥æ›´çŸ­ï¼‰
        self._max_retries = 2
        self._retry_delay = 2.0
        
        self.logger.info(f"åˆå§‹åŒ–æ± åŒ–æ•°æ®æºï¼Œæ± å¤§å°: {pool_min_size}-{pool_max_size}, å¹¶å‘: {self.max_concurrent_requests}")
    
    def is_available(self) -> bool:
        """
        æ£€æŸ¥Playwrightæ•°æ®æºæ˜¯å¦å¯ç”¨
        
        å¯¹äºPlaywrightæ•°æ®æºï¼Œå³ä½¿åˆå§‹åŒ–å¤±è´¥ä¹Ÿåº”è¯¥ä¿æŒå¯ç”¨çŠ¶æ€ä»¥ä¾¿é‡è¯•
        é™¤éå¥åº·çŠ¶æ€è¢«æ˜ç¡®æ ‡è®°ä¸ºFalseï¼ˆä¾‹å¦‚ç”±äºè¿‡å¤šè¿ç»­é”™è¯¯ï¼‰
        """
        from datetime import datetime
        
        # æ£€æŸ¥æ˜¯å¦åœ¨é€Ÿç‡é™åˆ¶æœŸé—´
        if self._rate_limit_reset and datetime.now() < self._rate_limit_reset:
            return False
        
        # å¯¹äºPlaywrightæ•°æ®æºï¼Œä¿æŒæ›´å®½æ¾çš„å¯ç”¨æ€§æ£€æŸ¥
        # å…è®¸åˆå§‹åŒ–å¤±è´¥åçš„é‡è¯•
        return self._healthy
    
    async def initialize(self):
        """åˆå§‹åŒ–æµè§ˆå™¨æ± ï¼ˆåº”åœ¨æœåŠ¡å¯åŠ¨æ—¶è°ƒç”¨ï¼‰"""
        if not self._pool_initialized:
            self.logger.info("æ­£åœ¨é¢„åˆå§‹åŒ–æµè§ˆå™¨æ± ...")
            await self._ensure_pool_initialized()
            self.logger.info("æµè§ˆå™¨æ± é¢„åˆå§‹åŒ–å®Œæˆ")
    
    async def _ensure_pool_initialized(self):
        """ç¡®ä¿æµè§ˆå™¨æ± å·²åˆå§‹åŒ–ï¼ˆè·¨è¿›ç¨‹å®‰å…¨ï¼‰"""
        process_id = os.getpid()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åˆå§‹åŒ–
        if (self._pool_initialized and 
            hasattr(self, '_init_pid') and 
            self._init_pid == process_id and 
            self._browser_pool):
            self.logger.debug(f"æµè§ˆå™¨æ± å·²åˆå§‹åŒ– (PID: {process_id})")
            return
        
        # éœ€è¦åˆå§‹åŒ–çš„æƒ…å†µå¤„ç†
        if not self._pool_initialized:
            self.logger.debug("æµè§ˆå™¨æ± æœªåˆå§‹åŒ–")
        elif not hasattr(self, '_init_pid') or self._init_pid != process_id:
            self.logger.info(f"è¿›ç¨‹forkæ£€æµ‹åˆ°ï¼Œé‡æ–°åˆå§‹åŒ–æµè§ˆå™¨æ±  (PID: {getattr(self, '_init_pid', 'None')} -> {process_id})")
        elif not self._browser_pool:
            self.logger.debug("æµè§ˆå™¨æ± å®ä¾‹ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°åˆ›å»º")
        
        try:
            self.logger.info(f"åˆå§‹åŒ–æµè§ˆå™¨æ±  (PID: {process_id})")
            self._browser_pool = BrowserPool(
                min_size=self.pool_min_size,
                max_size=self.pool_max_size
            )
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨è´¦æˆ·ç®¡ç†
            from ...config import Config
            if Config.ACCOUNT_MANAGEMENT_ENABLED:
                await self._browser_pool.initialize_with_account_manager()
                self.logger.info("âœ… æµè§ˆå™¨æ± å·²å¯ç”¨è´¦æˆ·ç®¡ç†åŠŸèƒ½")
            else:
                await self._browser_pool.initialize()
                self.logger.info("âš ï¸  æµè§ˆå™¨æ± æœªå¯ç”¨è´¦æˆ·ç®¡ç†åŠŸèƒ½")
            
            self._pool_initialized = True
            self._init_pid = process_id
            self.logger.info(f"æµè§ˆå™¨æ± åˆå§‹åŒ–å®Œæˆï¼Œæ± å¤§å°: {self.pool_min_size}-{self.pool_max_size}")
        except Exception as e:
            self.logger.error(f"æµè§ˆå™¨æ± åˆå§‹åŒ–å¤±è´¥: {e}")
            self._pool_initialized = False
            self._browser_pool = None
            raise
    
    async def _rate_limit(self):
        """è½»é‡çº§é™æµï¼ˆæ± åŒ–åé™æµå¯ä»¥æ›´å®½æ¾ï¼‰"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.request_interval:
            sleep_time = self.request_interval - time_since_last
            await asyncio.sleep(sleep_time)
        
        self.last_request_time = time.time()
    
    async def get_comprehensive_data(self, tweet_url: str) -> Dict[str, Any]:
        """
        è·å–æ¨æ–‡é¡µé¢çš„ç»¼åˆæ•°æ®
        
        Args:
            tweet_url: æ¨æ–‡URL
            
        Returns:
            åŒ…å«æ‰€æœ‰æå–æ•°æ®çš„å­—å…¸
        """
        # å»é‡æ£€æŸ¥
        if tweet_url in self._active_requests:
            self.logger.debug(f"è¯·æ±‚å»é‡: {tweet_url}")
            # ç®€å•ç­‰å¾…ç­–ç•¥ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥å®ç°æ›´å¤æ‚çš„ç­‰å¾…/å…±äº«æœºåˆ¶
            await asyncio.sleep(0.5)
            return await self.get_comprehensive_data(tweet_url)
        
        tweet_id = self._extract_tweet_id_from_url(tweet_url)
        
        async with self._semaphore:  # å¹¶å‘æ§åˆ¶
            self.logger.info(f"å¼€å§‹æ± åŒ–æå–: {tweet_url}")
            
            self._active_requests.add(tweet_url)
            try:
                await self._ensure_pool_initialized()
                data = await self._extract_comprehensive_data_pooled(tweet_url, tweet_id)
                self.handle_success()
                return data
            except Exception as e:
                error = DataSourceError(f"æ± åŒ–æå–å¤±è´¥: {str(e)}")
                self.handle_error(error)
                raise error
            finally:
                self._active_requests.discard(tweet_url)
    
    async def _extract_comprehensive_data_pooled(self, url: str, target_tweet_id: str = None) -> Dict[str, Any]:
        """ä½¿ç”¨æµè§ˆå™¨æ± è¿›è¡Œæ•°æ®æå–"""
        start_time = time.time()
        instance = None
        context = None
        page = None
        
        try:
            await self._rate_limit()
            
            # ä»æ± ä¸­è·å–æµè§ˆå™¨å®ä¾‹
            self.logger.info(f"æ­£åœ¨ä»æµè§ˆå™¨æ± è·å–å®ä¾‹...")
            
            # å…ˆæ£€æŸ¥æ± çŠ¶æ€
            pool_status = await self._browser_pool.get_pool_status()
            self.logger.info(f"æµè§ˆå™¨æ± çŠ¶æ€: å®ä¾‹æ•°={pool_status.get('total_instances', 0)}, "
                           f"å¯ç”¨æ•°={pool_status.get('available_instances', 0)}")
            
            # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°30ç§’
            instance, context, page = await self._browser_pool.acquire_instance(timeout=30.0)
            self.logger.info(f"âœ… æˆåŠŸè·å–æµè§ˆå™¨å®ä¾‹: {instance.instance_id if instance else 'None'}")
            
            # è®¤è¯ç”±æµè§ˆå™¨å®ä¾‹å†…éƒ¨å¤„ç†ï¼Œæ— éœ€é¢å¤–cookieåŠ è½½
            self.logger.debug("ä½¿ç”¨è´¦æˆ·ç®¡ç†ç³»ç»Ÿç»Ÿä¸€è®¤è¯")
            
            # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º
            from ..browser_pool.anti_detection import AntiDetectionManager
            anti_detection = AntiDetectionManager()
            
            if anti_detection.should_add_human_delay():
                delay = anti_detection.get_random_delay(0.5, 2.0)
                self.logger.debug(f"æ·»åŠ äººç±»è¡Œä¸ºå»¶è¿Ÿ: {delay:.2f}s")
                await asyncio.sleep(delay)
            
            # å¯¼èˆªåˆ°é¡µé¢
            self.logger.info(f"æ­£åœ¨å¯¼èˆªåˆ°é¡µé¢: {url}")
            response = await page.goto(url, wait_until='domcontentloaded', timeout=20000)
            self.logger.info(f"é¡µé¢å¯¼èˆªå®Œæˆï¼ŒçŠ¶æ€ç : {response.status if response else 'None'}")
            
            if response and response.status >= 400:
                raise DataSourceError(f"HTTPé”™è¯¯: {response.status}")
            
            # éªŒè¯é¡µé¢
            current_url = page.url
            if not any(domain in current_url for domain in ['x.com', 'twitter.com']):
                raise DataSourceError(f"å¯¼èˆªå¤±è´¥ï¼Œå½“å‰URL: {current_url}")
            
            # ä½¿ç”¨æå–å™¨ï¼Œæ·»åŠ è¶…æ—¶ä¿æŠ¤
            extractor = TweetDataExtractor(page)
            self.logger.info("å¼€å§‹æ•°æ®æå–")
            
            try:
                # ä»ç¯å¢ƒå˜é‡è·å–è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤30ç§’
                import os
                from dotenv import load_dotenv
                load_dotenv()  # ç¡®ä¿åŠ è½½.envæ–‡ä»¶
                timeout = float(os.getenv('PLAYWRIGHT_EXTRACTION_TIMEOUT', '30.0'))
                
                comprehensive_data = await asyncio.wait_for(
                    extractor.extract_all_data(target_tweet_id),
                    timeout=timeout
                )
                self.logger.info("æ•°æ®æå–å®Œæˆ")
            except asyncio.TimeoutError:
                # è¶…æ—¶å¯èƒ½æ˜¯é£æ§å¼•èµ·çš„ï¼Œä½¿ç”¨é£æ§æ£€æµ‹å™¨åˆ¤æ–­
                timeout_error = f"æ•°æ®æå–è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰"
                if rate_limit_detector.is_rate_limited(timeout_error):
                    self.logger.warning(f"ğŸš¨ æ£€æµ‹åˆ°å¯èƒ½çš„é£æ§å¼•èµ·çš„è¶…æ—¶: {url}")
                    try:
                        # ä½¿ç”¨é£æ§æ£€æµ‹å™¨çš„å®‰å…¨ç­‰å¾…æ–¹æ³•
                        await rate_limit_detector.safe_wait_for_selector(
                            page, '[data-testid="tweet"]', timeout=5000
                        )
                        # å¦‚æœæˆåŠŸäº†ï¼Œé‡è¯•æ•°æ®æå–
                        comprehensive_data = await asyncio.wait_for(
                            extractor.extract_all_data(target_tweet_id),
                            timeout=timeout
                        )
                        self.logger.info("é£æ§å¤„ç†åæ•°æ®æå–å®Œæˆ")
                    except Exception as rate_limit_error:
                        # å¦‚æœæ˜¯é£æ§å¼‚å¸¸ï¼Œè®©å®ƒä¼ æ’­åˆ°ä¸Šå±‚
                        if (hasattr(rate_limit_error, 'wait_time') and 
                            type(rate_limit_error).__name__ == 'RateLimitDetectedError'):
                            raise rate_limit_error
                        else:
                            raise DataSourceError(timeout_error)
                else:
                    raise DataSourceError(timeout_error)
            
            # æ·»åŠ æå–å…ƒæ•°æ®
            metadata = {
                'source': 'PlaywrightPooled',
                'instance_id': instance.instance_id if instance else 'unknown',
                'page_load_time': f"{(time.time() - start_time):.2f}s",
                'final_url': current_url,
                'pool_size': len(self._browser_pool.instances) if self._browser_pool else 0
            }
            
            # æ·»åŠ æµè§ˆå™¨å®ä¾‹ä¿¡æ¯ï¼ˆåŒ…å«è´¦æˆ·ç®¡ç†ä¿¡æ¯ï¼‰
            if instance:
                browser_info = {
                    'instance_id': instance.instance_id,
                    'current_account': instance.current_account.username if instance.current_account else None,
                    'account_usage_count': instance.account_usage_count,
                    'account_switch_threshold': instance.account_switch_threshold,
                    'using_env_cookie': instance.using_env_cookie
                }
                comprehensive_data['browser_info'] = browser_info
                self.logger.debug(f"è´¦æˆ·ä¿¡æ¯: {browser_info}")
            
            comprehensive_data['extraction_metadata'].update(metadata)
            
            tweet_count = (
                (1 if comprehensive_data.get('primary_tweet') else 0) +
                len(comprehensive_data.get('thread_tweets', [])) +
                len(comprehensive_data.get('related_tweets', []))
            )
            
            self.logger.info(f"æ± åŒ–æå–å®Œæˆ: {tweet_count} æ¡æ¨æ–‡, ç”¨æ—¶ {time.time() - start_time:.2f}s")
            return comprehensive_data
            
        except Exception as e:
            # æ£€æŸ¥æ˜¯å¦æ˜¯é£æ§å¼‚å¸¸
            if (hasattr(e, 'wait_time') and 
                type(e).__name__ == 'RateLimitDetectedError'):
                self.logger.warning(f"ğŸš¨ æ± åŒ–æå–æ£€æµ‹åˆ°é£æ§: {e}")
                # é£æ§å¼‚å¸¸éœ€è¦ä¼ æ’­åˆ°ä¸Šå±‚å¤„ç†ï¼Œä½†ä¸ç®—å®ä¾‹å¤±è´¥
                if instance:
                    await self._browser_pool.release_instance(instance, success=True)
                    instance = None
                raise  # ä¼ æ’­é£æ§å¼‚å¸¸
            else:
                self.logger.error(f"æ± åŒ–æå–å¤±è´¥: {e}")
                if instance:
                    await self._browser_pool.release_instance(instance, success=False)
                    instance = None  # é¿å…é‡å¤é‡Šæ”¾
                raise
        finally:
            # é‡Šæ”¾æµè§ˆå™¨å®ä¾‹
            if instance:
                await self._browser_pool.release_instance(instance, success=True)
    
    async def get_tweet_data(self, tweet_id: str) -> TweetData:
        """è·å–å•æ¡æ¨æ–‡æ•°æ®"""
        tweet_id = self._extract_tweet_id(tweet_id)
        
        if not self._validate_tweet_id(tweet_id):
            raise ValueError(f"æ— æ•ˆçš„æ¨æ–‡ID: {tweet_id}")
        
        tweet_url = f"https://x.com/i/web/status/{tweet_id}"
        comprehensive_data = await self.get_comprehensive_data(tweet_url)
        
        # æå–ä¸»æ¨æ–‡
        primary_tweet = comprehensive_data.get('primary_tweet')
        if not primary_tweet:
            raise NotFoundError(f"æ¨æ–‡ {tweet_id} æœªæ‰¾åˆ°")
        
        return self._convert_to_tweet_data(primary_tweet, tweet_id)
    
    async def batch_get_tweet_data(self, tweet_ids: List[str]) -> List[TweetData]:
        """
        å¹¶å‘æ‰¹é‡è·å–æ¨æ–‡æ•°æ®
        
        è¿™æ˜¯æ± åŒ–æ•°æ®æºçš„ä¸»è¦ä¼˜åŠ¿ - å¯ä»¥å¹¶è¡Œå¤„ç†å¤šä¸ªè¯·æ±‚
        """
        if not tweet_ids:
            return []
        
        self.logger.info(f"å¼€å§‹å¹¶å‘æ‰¹é‡å¤„ç† {len(tweet_ids)} æ¡æ¨æ–‡")
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        for tweet_id in tweet_ids:
            cleaned_id = self._extract_tweet_id(tweet_id)
            if self._validate_tweet_id(cleaned_id):
                task = self._get_single_tweet_with_retry(cleaned_id)
                tasks.append(task)
        
        if not tasks:
            return []
        
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è¿‡æ»¤ç»“æœ
        successful_results = []
        failed_count = 0
        
        for result in results:
            if isinstance(result, Exception):
                self.logger.warning(f"æ‰¹é‡å¤„ç†ä¸­çš„å¤±è´¥: {result}")
                failed_count += 1
            elif result:
                successful_results.append(result)
        
        self.logger.info(f"å¹¶å‘æ‰¹é‡å¤„ç†å®Œæˆ: {len(successful_results)} æˆåŠŸ, {failed_count} å¤±è´¥")
        return successful_results
    
    async def _get_single_tweet_with_retry(self, tweet_id: str) -> Optional[TweetData]:
        """å¸¦é‡è¯•çš„å•æ¨æ–‡è·å–"""
        for attempt in range(self._max_retries + 1):
            try:
                return await self.get_tweet_data(tweet_id)
            except Exception as e:
                if attempt < self._max_retries:
                    self.logger.warning(f"æ¨æ–‡ {tweet_id} è·å–å¤±è´¥ (å°è¯• {attempt + 1}/{self._max_retries + 1}): {e}")
                    await asyncio.sleep(self._retry_delay * (attempt + 1))
                else:
                    self.logger.error(f"æ¨æ–‡ {tweet_id} æœ€ç»ˆè·å–å¤±è´¥: {e}")
                    return None
    
    async def get_user_tweets(self, username: str, max_results: int = 10) -> List[TweetData]:
        """è·å–ç”¨æˆ·æ¨æ–‡"""
        username = username.lstrip('@')
        
        if not self._validate_username(username):
            raise ValueError(f"æ— æ•ˆçš„ç”¨æˆ·å: {username}")
        
        profile_url = f"https://x.com/{username}"
        
        try:
            comprehensive_data = await self.get_comprehensive_data(profile_url)
            
            # æå–è¯¥ç”¨æˆ·çš„æ¨æ–‡
            user_tweets = []
            
            # æ£€æŸ¥ä¸»æ¨æ–‡
            primary_tweet = comprehensive_data.get('primary_tweet')
            if (primary_tweet and 
                primary_tweet.get('author', {}).get('username') == username):
                user_tweets.append(self._convert_to_tweet_data(primary_tweet))
            
            # æ£€æŸ¥çº¿ç¨‹æ¨æ–‡
            for tweet in comprehensive_data.get('thread_tweets', []):
                if tweet.get('author', {}).get('username') == username:
                    user_tweets.append(self._convert_to_tweet_data(tweet))
            
            # æ£€æŸ¥ç›¸å…³æ¨æ–‡
            for tweet in comprehensive_data.get('related_tweets', []):
                if tweet.get('author', {}).get('username') == username:
                    user_tweets.append(self._convert_to_tweet_data(tweet))
            
            return user_tweets[:max_results]
            
        except Exception as e:
            self.logger.error(f"è·å–ç”¨æˆ· @{username} æ¨æ–‡å¤±è´¥: {e}")
            return []
    
    async def get_user_data(self, username: str) -> UserData:
        """è·å–ç”¨æˆ·æ•°æ®"""
        username = username.lstrip('@')
        
        if not self._validate_username(username):
            raise ValueError(f"æ— æ•ˆçš„ç”¨æˆ·å: {username}")
        
        profile_url = f"https://x.com/{username}"
        
        try:
            comprehensive_data = await self.get_comprehensive_data(profile_url)
            
            # ä»æ¨æ–‡ä¸­æå–ç”¨æˆ·ä¿¡æ¯
            user_info = None
            for tweet_source in ['primary_tweet', 'thread_tweets', 'related_tweets']:
                if tweet_source == 'primary_tweet':
                    tweet = comprehensive_data.get(tweet_source)
                    if tweet and tweet.get('author', {}).get('username') == username:
                        user_info = tweet['author']
                        break
                else:
                    tweets = comprehensive_data.get(tweet_source, [])
                    for tweet in tweets:
                        if tweet.get('author', {}).get('username') == username:
                            user_info = tweet['author']
                            break
                    if user_info:
                        break
            
            if not user_info:
                raise NotFoundError(f"ç”¨æˆ· @{username} æœªæ‰¾åˆ°")
            
            return UserData(
                user_id=user_info.get('username', username),
                username=user_info.get('username', username),
                name=user_info.get('display_name', username),
                description=None,
                public_metrics={
                    'followers_count': 0,
                    'following_count': 0,
                    'tweet_count': 0,
                    'listed_count': 0
                },
                profile_image_url=user_info.get('avatar_url'),
                verified=user_info.get('is_verified', False)
            )
            
        except Exception as e:
            error = DataSourceError(f"è·å–ç”¨æˆ· @{username} æ•°æ®å¤±è´¥: {str(e)}")
            self.handle_error(error)
            raise error
    
    async def search_tweets(self, query: str, max_results: int = 10) -> List[TweetData]:
        """æœç´¢æ¨æ–‡"""
        if not query or not query.strip():
            raise ValueError("æœç´¢æŸ¥è¯¢ä¸èƒ½ä¸ºç©º")
        
        search_url = f"https://x.com/search?q={query.replace(' ', '%20')}&src=typed_query"
        
        try:
            comprehensive_data = await self.get_comprehensive_data(search_url)
            
            # æå–æœç´¢ç»“æœ
            search_results = []
            
            for tweet_source in ['primary_tweet', 'thread_tweets', 'related_tweets']:
                if tweet_source == 'primary_tweet':
                    tweet = comprehensive_data.get(tweet_source)
                    if tweet:
                        search_results.append(self._convert_to_tweet_data(tweet))
                else:
                    tweets = comprehensive_data.get(tweet_source, [])
                    for tweet in tweets:
                        search_results.append(self._convert_to_tweet_data(tweet))
            
            return search_results[:max_results]
            
        except Exception as e:
            self.logger.error(f"æœç´¢æ¨æ–‡å¤±è´¥ '{query}': {e}")
            return []
    
    async def get_pool_status(self) -> Dict[str, Any]:
        """è·å–æµè§ˆå™¨æ± çŠ¶æ€"""
        if not self._browser_pool or not self._pool_initialized:
            return {
                'initialized': False,
                'error': 'æµè§ˆå™¨æ± æœªåˆå§‹åŒ–'
            }
        
        return await self._browser_pool.get_pool_status()
    
    def _convert_to_tweet_data(self, tweet_dict: Dict[str, Any], fallback_id: str = None) -> TweetData:
        """è½¬æ¢æ¨æ–‡æ•°æ®æ ¼å¼"""
        author = tweet_dict.get('author', {})
        metrics = tweet_dict.get('metrics', {})
        
        return TweetData(
            tweet_id=tweet_dict.get('tweet_id') or fallback_id or 'unknown',
            text=tweet_dict.get('text', ''),
            author_username=author.get('username', 'unknown'),
            author_name=author.get('display_name', 'Unknown'),
            created_at=tweet_dict.get('timestamp') or datetime.now().isoformat(),
            public_metrics={
                'retweet_count': metrics.get('retweets', 0),
                'like_count': metrics.get('likes', 0),
                'reply_count': metrics.get('replies', 0),
                'quote_count': metrics.get('quotes', 0)
            },
            view_count=metrics.get('views'),
            url=f"https://twitter.com/{author.get('username', 'unknown')}/status/{tweet_dict.get('tweet_id', 'unknown')}",
            lang=tweet_dict.get('language')
        )
    
    def _extract_tweet_id_from_url(self, url: str) -> Optional[str]:
        """ä»URLæå–æ¨æ–‡ID"""
        import re
        match = re.search(r'/status/(\d+)', url)
        return match.group(1) if match else None
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self._browser_pool:
            await self._browser_pool.dispose()
            self._browser_pool = None
            self._pool_initialized = False
        
        self.logger.info("æ± åŒ–æ•°æ®æºå·²æ¸…ç†")
    
    def __del__(self):
        """ææ„æ—¶ç¡®ä¿èµ„æºæ¸…ç†"""
        if self._browser_pool and self._pool_initialized:
            # æ³¨æ„ï¼šåœ¨ææ„å‡½æ•°ä¸­ä¸èƒ½ä½¿ç”¨async/await
            self.logger.warning("æ£€æµ‹åˆ°æœªæ¸…ç†çš„æµè§ˆå™¨æ± ï¼Œè¯·è°ƒç”¨cleanup()æ–¹æ³•")