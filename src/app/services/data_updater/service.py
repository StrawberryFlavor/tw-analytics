"""
æ•°æ®æ›´æ–°æœåŠ¡

éµå¾ªSOLIDåŸåˆ™çš„æ ¸å¿ƒæ•°æ®æ›´æ–°æœåŠ¡
è´Ÿè´£åè°ƒæ•°æ®åº“æŸ¥è¯¢ã€Twitter APIè°ƒç”¨ã€æ‰¹å¤„ç†ç®¡ç†å’Œè¿›åº¦è¿½è¸ª
ä¸“ä¸º732æ¡è®°å½•çš„é«˜æ•ˆæ›´æ–°è®¾è®¡
"""

import asyncio
import logging
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, AsyncGenerator, Callable
from dataclasses import dataclass

from ..database import DatabaseService, CampaignTweetSnapshot, CampaignTweetSnapshotQuery
from ..twitter.service import TwitterService
from ...core.config_factory import get_updater_config, UpdaterConfig
from .batch_manager import BatchManager, BatchInfo, BatchResult, BatchStrategy
from .rate_limiter import BatchRateLimiter
from .progress_tracker import ProgressTracker, UpdateStatus, RecordProgress


@dataclass
class UpdateResult:
    """æ›´æ–°ç»“æœæ•°æ®ç±»"""
    total_records: int
    processed_records: int
    successful_updates: int
    failed_updates: int
    skipped_records: int
    processing_time: float
    success_rate: float
    errors: List[str]
    session_id: str


class TweetDataUpdater:
    """æ¨æ–‡æ•°æ®æ›´æ–°å™¨ - å•ä¸€èŒè´£åŸåˆ™"""
    
    def __init__(self, 
                 database_service: DatabaseService,
                 twitter_service: TwitterService,
                 config: UpdaterConfig = None):
        """
        åˆå§‹åŒ–æ¨æ–‡æ•°æ®æ›´æ–°å™¨
        
        Args:
            database_service: æ•°æ®åº“æœåŠ¡
            twitter_service: TwitteræœåŠ¡
            config: æ›´æ–°é…ç½®
        """
        self.db_service = database_service
        self.twitter_service = twitter_service
        self.config = config or get_updater_config('safe')
        
        # éªŒè¯é…ç½®
        is_valid, error_msg = self.config.validate()
        if not is_valid:
            raise ValueError(f"é…ç½®æ— æ•ˆ: {error_msg}")
        
        self.logger = logging.getLogger(__name__)
        
        # æ ¸å¿ƒç»„ä»¶
        self.rate_limiter = BatchRateLimiter(
            requests_per_minute=self.config.requests_per_minute,
            requests_per_hour=self.config.requests_per_hour,
            base_delay=self.config.request_delay_seconds,
            batch_delay=self.config.batch_delay_seconds
        )
        
        self.batch_manager = BatchManager(
            config=self.config,
            rate_limiter=self.rate_limiter
        )
        
        self.progress_tracker: Optional[ProgressTracker] = None
        
        self.logger.info(f"ğŸš€ æ•°æ®æ›´æ–°å™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ğŸ“‹ é…ç½®: {self.config}")
    
    async def update_all_records(self, 
                               filter_recent: bool = None,
                               batch_strategy: str = "equal",
                               progress_file: str = None) -> UpdateResult:
        """
        æ›´æ–°æ‰€æœ‰éœ€è¦æ›´æ–°çš„è®°å½•
        
        Args:
            filter_recent: æ˜¯å¦è¿‡æ»¤æœ€è¿‘å·²æ›´æ–°çš„è®°å½•
            batch_strategy: æ‰¹å¤„ç†ç­–ç•¥ ("equal", "priority", "adaptive")
            progress_file: è¿›åº¦æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ›´æ–°ç»“æœ
        """
        start_time = time.time()
        
        # åˆå§‹åŒ–è¿›åº¦è¿½è¸ª
        session_id = f"update_all_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.progress_tracker = ProgressTracker(
            session_id=session_id,
            progress_file_path=progress_file or f"progress/{session_id}.json",
            auto_save_interval=self.config.progress_save_interval
        )
        
        try:
            self.logger.info("ğŸ” å¼€å§‹å…¨é‡æ•°æ®æ›´æ–°...")
            
            # ç¬¬1æ­¥: è·å–éœ€è¦æ›´æ–°çš„è®°å½•
            self.progress_tracker.update_phase("æŸ¥è¯¢éœ€è¦æ›´æ–°çš„è®°å½•")
            
            if filter_recent is None:
                filter_recent = self.config.skip_recent_updates
            
            records_to_update = await self._get_records_to_update(filter_recent)
            
            if not records_to_update:
                self.logger.info("âœ… æ²¡æœ‰éœ€è¦æ›´æ–°çš„è®°å½•")
                return UpdateResult(
                    total_records=0,
                    processed_records=0,
                    successful_updates=0,
                    failed_updates=0,
                    skipped_records=0,
                    processing_time=time.time() - start_time,
                    success_rate=100.0,
                    errors=[],
                    session_id=session_id
                )
            
            self.logger.info(f"ğŸ“Š æ‰¾åˆ° {len(records_to_update)} æ¡éœ€è¦æ›´æ–°çš„è®°å½•")
            
            # ç¬¬2æ­¥: åˆ›å»ºæ‰¹æ¬¡
            self.progress_tracker.update_phase("åˆ›å»ºå¤„ç†æ‰¹æ¬¡")
            
            batches = self.batch_manager.create_batches(
                records=records_to_update,
                strategy=batch_strategy
            )
            
            # åˆå§‹åŒ–è¿›åº¦è¿½è¸ªä¼šè¯
            self.progress_tracker.initialize_session(
                total_records=len(records_to_update),
                total_batches=len(batches)
            )
            
            # ç¬¬3æ­¥: å¤„ç†æ‰¹æ¬¡
            self.progress_tracker.update_phase("æ‰¹é‡å¤„ç†æ•°æ®")
            
            batch_results = await self.batch_manager.process_batches(
                batches=batches,
                batch_processor=self._process_batch,
                progress_callback=self._on_batch_complete
            )
            
            # ç¬¬4æ­¥: æ±‡æ€»ç»“æœ
            result = self._create_update_result(
                records_to_update=records_to_update,
                batch_results=batch_results,
                processing_time=time.time() - start_time,
                session_id=session_id
            )
            
            # å®Œæˆä¼šè¯
            final_status = UpdateStatus.COMPLETED if result.success_rate > 50 else UpdateStatus.FAILED
            self.progress_tracker.complete_session(final_status)
            
            self.logger.info("âœ… å…¨é‡æ•°æ®æ›´æ–°å®Œæˆ!")
            self.logger.info(f"ğŸ“Š æœ€ç»ˆç»“æœ: {result.successful_updates}/{result.total_records} æˆåŠŸ "
                           f"({result.success_rate:.1f}%)")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ æ•°æ®æ›´æ–°è¿‡ç¨‹å¼‚å¸¸: {e}")
            if self.progress_tracker:
                self.progress_tracker.complete_session(UpdateStatus.FAILED)
            raise
    
    async def update_specific_records(self, 
                                    record_ids: List[int],
                                    progress_file: str = None) -> UpdateResult:
        """
        æ›´æ–°æŒ‡å®šçš„è®°å½•
        
        Args:
            record_ids: è¦æ›´æ–°çš„è®°å½•IDåˆ—è¡¨
            progress_file: è¿›åº¦æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ›´æ–°ç»“æœ
        """
        start_time = time.time()
        
        # åˆå§‹åŒ–è¿›åº¦è¿½è¸ª
        session_id = f"update_specific_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.progress_tracker = ProgressTracker(
            session_id=session_id,
            progress_file_path=progress_file or f"progress/{session_id}.json",
            auto_save_interval=self.config.progress_save_interval
        )
        
        try:
            self.logger.info(f"ğŸ” å¼€å§‹æ›´æ–°æŒ‡å®šè®°å½•: {len(record_ids)} æ¡")
            
            # è·å–è®°å½•
            self.progress_tracker.update_phase("æŸ¥è¯¢æŒ‡å®šè®°å½•")
            
            records = []
            for record_id in record_ids:
                record = await self.db_service.get_by_id(record_id)
                if record:
                    records.append(record)
                else:
                    self.logger.warning(f"è®°å½• {record_id} æœªæ‰¾åˆ°")
            
            if not records:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è®°å½•")
                return UpdateResult(
                    total_records=len(record_ids),
                    processed_records=0,
                    successful_updates=0,
                    failed_updates=0,
                    skipped_records=len(record_ids),
                    processing_time=time.time() - start_time,
                    success_rate=0.0,
                    errors=["æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆè®°å½•"],
                    session_id=session_id
                )
            
            # åˆ›å»ºæ‰¹æ¬¡å¹¶å¤„ç†
            batches = self.batch_manager.create_batches(records, "equal")
            self.progress_tracker.initialize_session(len(records), len(batches))
            
            batch_results = await self.batch_manager.process_batches(
                batches=batches,
                batch_processor=self._process_batch,
                progress_callback=self._on_batch_complete
            )
            
            # æ±‡æ€»ç»“æœ
            result = self._create_update_result(
                records_to_update=records,
                batch_results=batch_results,
                processing_time=time.time() - start_time,
                session_id=session_id
            )
            
            final_status = UpdateStatus.COMPLETED if result.success_rate > 50 else UpdateStatus.FAILED
            self.progress_tracker.complete_session(final_status)
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ æŒ‡å®šè®°å½•æ›´æ–°å¼‚å¸¸: {e}")
            if self.progress_tracker:
                self.progress_tracker.complete_session(UpdateStatus.FAILED)
            raise
    
    async def _get_records_to_update(self, filter_recent: bool = True) -> List[CampaignTweetSnapshot]:
        """è·å–éœ€è¦æ›´æ–°çš„è®°å½•"""
        
        # æ„å»ºæŸ¥è¯¢ - æŸ¥æ‰¾ç¼ºå¤±å­—æ®µçš„è®°å½•
        query_builder = CampaignTweetSnapshotQuery()
        
        # æ·»åŠ æ¡ä»¶ï¼šç¼ºå¤±å…³é”®å­—æ®µ
        query_builder.where("""
            (author_name IS NULL OR author_name = '' OR
             tweet_time_utc IS NULL OR
             views IS NULL)
        """)
        
        # å¦‚æœå¯ç”¨è¿‡æ»¤æœ€è¿‘æ›´æ–°
        if filter_recent:
            threshold_time = datetime.now() - timedelta(hours=self.config.recent_update_threshold_hours)
            query_builder.where("created_at < %s", threshold_time)
        
        # æŒ‰IDæ’åºï¼Œç¡®ä¿ä¸€è‡´çš„å¤„ç†é¡ºåº
        query_builder.order_by("id", "ASC")
        
        # æ‰§è¡ŒæŸ¥è¯¢
        records = await self.db_service.execute_custom_query(query_builder)
        
        self.logger.info(f"ğŸ” æŸ¥è¯¢æ¡ä»¶: ç¼ºå¤±å…³é”®å­—æ®µçš„è®°å½•")
        if filter_recent:
            self.logger.info(f"ğŸ“… æ’é™¤æœ€è¿‘ {self.config.recent_update_threshold_hours} å°æ—¶å†…çš„è®°å½•")
        
        return records
    
    async def _process_batch(self, batch_info: BatchInfo) -> AsyncGenerator[BatchResult, None]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡ - å¼‚æ­¥ç”Ÿæˆå™¨"""
        
        self.progress_tracker.start_batch(batch_info)
        batch_start_time = time.time()
        
        successful_records = []
        failed_records = []
        skipped_records = []
        errors = []
        
        self.logger.info(f"ğŸ“¦ å¼€å§‹å¤„ç†æ‰¹æ¬¡ {batch_info.batch_id}: {batch_info.size} æ¡è®°å½•")
        
        for record in batch_info.records:
            try:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                needs_update, missing_fields = self._check_record_needs_update(record)
                
                if not needs_update:
                    skipped_records.append(record)
                    self.progress_tracker.update_record_status(
                        record.id, UpdateStatus.SKIPPED, "è®°å½•å·²å®Œæ•´ï¼Œæ— éœ€æ›´æ–°")
                    continue
                
                # é€Ÿç‡é™åˆ¶æ§åˆ¶
                await self.rate_limiter.wait_if_needed()
                
                # æ›´æ–°è®°å½•
                update_start = time.time()
                success, updated_record, error = await self._update_single_record(record, missing_fields)
                update_time = time.time() - update_start
                
                if success:
                    successful_records.append(updated_record)
                    self.progress_tracker.update_record_status(
                        record.id, 
                        UpdateStatus.COMPLETED,
                        updated_fields=missing_fields,
                        processing_time=update_time
                    )
                    self.rate_limiter.record_request(True, update_time)
                    
                    self.logger.debug(f"âœ… è®°å½• {record.id} æ›´æ–°æˆåŠŸ: {missing_fields}")
                    
                else:
                    failed_records.append(record)
                    errors.append({
                        'record_id': record.id,
                        'tweet_id': record.tweet_id,
                        'error': error
                    })
                    self.progress_tracker.update_record_status(
                        record.id, 
                        UpdateStatus.FAILED,
                        error_message=error,
                        processing_time=update_time
                    )
                    self.rate_limiter.record_request(False, update_time, error)
                    
                    self.logger.warning(f"âŒ è®°å½• {record.id} æ›´æ–°å¤±è´¥: {error}")
                
            except Exception as e:
                failed_records.append(record)
                error_msg = f"å¤„ç†å¼‚å¸¸: {str(e)}"
                errors.append({
                    'record_id': record.id,
                    'tweet_id': record.tweet_id,
                    'error': error_msg
                })
                
                self.progress_tracker.update_record_status(
                    record.id, 
                    UpdateStatus.FAILED,
                    error_message=error_msg
                )
                self.rate_limiter.record_request(False, 0.0, "exception")
                
                self.logger.error(f"âŒ è®°å½• {record.id} å¤„ç†å¼‚å¸¸: {e}")
        
        # åˆ›å»ºæ‰¹æ¬¡ç»“æœ
        batch_result = BatchResult(
            batch_info=batch_info,
            success_count=len(successful_records),
            failure_count=len(failed_records),
            errors=errors,
            processing_time=time.time() - batch_start_time,
            updated_records=successful_records,
            skipped_records=skipped_records
        )
        
        success_rate = (len(successful_records) / batch_info.size * 100) if batch_info.size > 0 else 0
        self.logger.info(f"âœ… æ‰¹æ¬¡ {batch_info.batch_id} å¤„ç†å®Œæˆ: "
                        f"{len(successful_records)}/{batch_info.size} æˆåŠŸ ({success_rate:.1f}%)")
        
        yield batch_result
    
    def _check_record_needs_update(self, record: CampaignTweetSnapshot) -> tuple[bool, List[str]]:
        """æ£€æŸ¥è®°å½•æ˜¯å¦éœ€è¦æ›´æ–°"""
        missing_fields = []
        
        # æ£€æŸ¥å„ä¸ªå­—æ®µ
        if not record.author_name or record.author_name.strip() == "":
            missing_fields.append("author_name")
        
        if not record.tweet_time_utc:
            missing_fields.append("tweet_time_utc")
        
        # æ£€æŸ¥viewså­—æ®µï¼šå¦‚æœä¸ºNoneæˆ–0ï¼Œéœ€è¦æ›´æ–°
        if record.views is None or record.views == 0:
            missing_fields.append("views")
        
        return len(missing_fields) > 0, missing_fields
    
    async def _update_single_record(self, 
                                  record: CampaignTweetSnapshot, 
                                  missing_fields: List[str]) -> tuple[bool, CampaignTweetSnapshot, str]:
        """æ›´æ–°å•æ¡è®°å½•"""
        try:
            # æ„å»ºæ¨æ–‡URL
            tweet_url = f"https://twitter.com/{record.author_username}/status/{record.tweet_id}"
            
            # è°ƒç”¨comprehensive APIè·å–å®Œæ•´æ•°æ®
            comprehensive_data = await self.twitter_service.get_comprehensive_data(tweet_url)
            
            if not comprehensive_data or not comprehensive_data.get('primary_tweet'):
                return False, record, "æ— æ³•è·å–æ¨æ–‡ç»¼åˆæ•°æ®"
            
            primary_tweet = comprehensive_data['primary_tweet']
            updated_fields = []
            
            # æ·»åŠ è°ƒè¯•æ—¥å¿—
            self.logger.info(f"ğŸ” è°ƒè¯•ä¿¡æ¯ - è®°å½• {record.id}:")
            self.logger.info(f"   ç¼ºå¤±å­—æ®µ: {missing_fields}")
            self.logger.info(f"   primary_tweet keys: {list(primary_tweet.keys())}")
            
            # æ›´æ–°ç¼ºå¤±çš„å­—æ®µ
            if 'author_name' in missing_fields:
                author = primary_tweet.get('author', {})
                self.logger.info(f"   author æ•°æ®: {author}")
                
                # å°è¯•å¤šç§å¯èƒ½çš„author_nameå­—æ®µ
                author_name = None
                if isinstance(author, dict):
                    author_name = (
                        author.get('name') or 
                        author.get('display_name') or 
                        author.get('displayName') or
                        author.get('screen_name') or
                        author.get('username')
                    )
                
                self.logger.info(f"   æå–åˆ°çš„ author_name: {repr(author_name)}")
                
                if author_name:
                    record.author_name = str(author_name).strip()
                    updated_fields.append('author_name')
                    self.logger.info(f"   âœ… å°†æ›´æ–° author_name: {record.author_name}")
                else:
                    self.logger.warning(f"   âŒ æœªèƒ½æå–åˆ°æœ‰æ•ˆçš„ author_name")
            
            if 'tweet_time_utc' in missing_fields:
                timestamp = primary_tweet.get('timestamp') or primary_tweet.get('time')
                if timestamp:
                    # è½¬æ¢æ—¶é—´æˆ³æ ¼å¼
                    if isinstance(timestamp, str):
                        try:
                            # å°è¯•è§£æä¸åŒçš„æ—¶é—´æ ¼å¼
                            if 'T' in timestamp:
                                record.tweet_time_utc = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                            else:
                                record.tweet_time_utc = datetime.fromisoformat(timestamp)
                            updated_fields.append('tweet_time_utc')
                        except ValueError as e:
                            self.logger.warning(f"æ—¶é—´æ ¼å¼è§£æå¤±è´¥: {timestamp}, é”™è¯¯: {e}")
            
            # æ›´æ–°viewså­—æ®µ
            if 'views' in missing_fields:
                # å°è¯•ä»ä¸åŒçš„æ•°æ®ç»“æ„ä¸­æå–views
                views_value = None
                
                # 1. ä»metricsä¸­è·å–
                metrics = primary_tweet.get('metrics', {})
                if isinstance(metrics, dict):
                    views_value = (
                        metrics.get('views') or 
                        metrics.get('view_count') or
                        metrics.get('impressions')
                    )
                
                # 2. ç›´æ¥ä»primary_tweetä¸­è·å–
                if views_value is None:
                    views_value = (
                        primary_tweet.get('views') or
                        primary_tweet.get('view_count') or
                        primary_tweet.get('impressions') or
                        primary_tweet.get('engagement_stats', {}).get('views')
                    )
                
                self.logger.info(f"   æå–åˆ°çš„ views: {repr(views_value)}")
                
                # ç¡®ä¿viewsæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ•°å­—
                if views_value is not None:
                    try:
                        # å¤„ç†å­—ç¬¦ä¸²æ•°å­—ï¼ˆå¦‚"1.2K", "5M"ï¼‰
                        if isinstance(views_value, str):
                            views_value = views_value.replace(',', '').strip()
                            
                            # å¤„ç†K, Mç­‰åç¼€
                            if views_value.endswith('K') or views_value.endswith('k'):
                                views_value = float(views_value[:-1]) * 1000
                            elif views_value.endswith('M') or views_value.endswith('m'):
                                views_value = float(views_value[:-1]) * 1000000
                            else:
                                views_value = float(views_value)
                        
                        views_value = int(views_value)
                        
                        if views_value > 0:  # åªæœ‰å¤§äº0çš„viewsæ‰æ›´æ–°
                            record.views = views_value
                            updated_fields.append('views')
                            self.logger.info(f"   âœ… å°†æ›´æ–° views: {record.views}")
                        else:
                            self.logger.warning(f"   âš ï¸  è·å–åˆ°çš„viewså€¼ä¸º0æˆ–è´Ÿæ•°: {views_value}")
                    
                    except (ValueError, TypeError) as e:
                        self.logger.warning(f"   âŒ viewså€¼æ ¼å¼è½¬æ¢å¤±è´¥: {views_value}, é”™è¯¯: {e}")
                else:
                    self.logger.warning(f"   âŒ æœªèƒ½æå–åˆ°æœ‰æ•ˆçš„ views å€¼")
            
            # ä¿å­˜æ›´æ–°åˆ°æ•°æ®åº“
            if updated_fields:
                success = await self.db_service.update_record(record)
                if success:
                    return True, record, ""
                else:
                    return False, record, "æ•°æ®åº“æ›´æ–°å¤±è´¥"
            else:
                return False, record, "æ²¡æœ‰å¯æ›´æ–°çš„å­—æ®µ"
                
        except Exception as e:
            return False, record, f"æ›´æ–°è¿‡ç¨‹å¼‚å¸¸: {str(e)}"
    
    def _on_batch_complete(self, batch_num: int, total_batches: int, batch_result: BatchResult):
        """æ‰¹æ¬¡å®Œæˆå›è°ƒ"""
        self.progress_tracker.complete_batch(batch_result)
        
        progress = (batch_num / total_batches) * 100
        self.logger.info(f"ğŸ“Š è¿›åº¦: {batch_num}/{total_batches} æ‰¹æ¬¡ ({progress:.1f}%)")
    
    def _create_update_result(self, 
                            records_to_update: List[CampaignTweetSnapshot],
                            batch_results: List[BatchResult],
                            processing_time: float,
                            session_id: str) -> UpdateResult:
        """åˆ›å»ºæ›´æ–°ç»“æœ"""
        
        total_records = len(records_to_update)
        successful_updates = sum(r.success_count for r in batch_results)
        failed_updates = sum(r.failure_count for r in batch_results)
        skipped_records = sum(len(r.skipped_records) for r in batch_results)
        
        processed_records = successful_updates + failed_updates
        success_rate = (successful_updates / processed_records * 100) if processed_records > 0 else 0.0
        
        # æ”¶é›†æ‰€æœ‰é”™è¯¯
        all_errors = []
        for batch_result in batch_results:
            all_errors.extend([
                f"æ‰¹æ¬¡ {batch_result.batch_info.batch_id}: {error.get('error', 'Unknown')}"
                for error in batch_result.errors
            ])
        
        return UpdateResult(
            total_records=total_records,
            processed_records=processed_records,
            successful_updates=successful_updates,
            failed_updates=failed_updates,
            skipped_records=skipped_records,
            processing_time=processing_time,
            success_rate=success_rate,
            errors=all_errors,
            session_id=session_id
        )
    
    def get_progress_summary(self) -> Optional[Dict[str, Any]]:
        """è·å–è¿›åº¦æ‘˜è¦"""
        if self.progress_tracker:
            return self.progress_tracker.get_summary()
        return None
    
    def get_rate_limiter_stats(self) -> Dict[str, Any]:
        """è·å–é€Ÿç‡é™åˆ¶å™¨ç»Ÿè®¡"""
        return self.rate_limiter.get_statistics()
    
    def get_batch_manager_stats(self) -> Dict[str, Any]:
        """è·å–æ‰¹å¤„ç†ç®¡ç†å™¨ç»Ÿè®¡"""
        return self.batch_manager.get_statistics()


# ä¾¿æ·å‡½æ•°å’Œå·¥å‚æ–¹æ³•

async def create_data_updater(config: UpdaterConfig = None) -> TweetDataUpdater:
    """åˆ›å»ºæ•°æ®æ›´æ–°å™¨å®ä¾‹ - å·¥å‚æ–¹æ³•"""
    
    # å¯¼å…¥æœåŠ¡
    from ..database import get_database_service
    
    # è·å–æ•°æ®åº“æœåŠ¡
    db_service = await get_database_service()
    
    # ç‹¬ç«‹åˆ›å»ºTwitteræœåŠ¡ï¼ˆä¸ä¾èµ–Flaskå®¹å™¨ï¼‰
    twitter_service = await _create_standalone_twitter_service()
    
    # åˆ›å»ºæ›´æ–°å™¨
    updater = TweetDataUpdater(
        database_service=db_service,
        twitter_service=twitter_service,
        config=config or UpdaterConfig.create_safe_config()
    )
    
    return updater


async def _create_standalone_twitter_service():
    """ç‹¬ç«‹åˆ›å»ºTwitteræœåŠ¡ï¼ˆç”¨äºæ•°æ®æ›´æ–°è„šæœ¬ï¼‰"""
    import os
    from ..twitter.service import TwitterService
    from ..data_sources.manager import DataSourceManager
    from ..data_sources.playwright_pooled import PlaywrightPooledSource
    from ..data_sources.twitter_api import TwitterAPISource
    from ..data_sources.apify_source import ApifyTwitterSource
    from ..utils.async_runner import AsyncRunner
    
    # åˆ›å»ºæ•°æ®æº
    sources = []
    
    # 1. Playwrightæ•°æ®æº
    try:
        playwright_source = PlaywrightPooledSource(
            pool_min_size=int(os.getenv('BROWSER_POOL_MIN_SIZE', '2')),
            pool_max_size=int(os.getenv('BROWSER_POOL_MAX_SIZE', '4')),  # æ•°æ®æ›´æ–°æ—¶å‡å°‘å¹¶å‘
            max_concurrent_requests=int(os.getenv('BROWSER_POOL_MAX_CONCURRENT_REQUESTS', '2'))
        )
        sources.append(playwright_source)
    except Exception as e:
        logging.warning(f"æ— æ³•åˆ›å»ºPlaywrightæ•°æ®æº: {e}")
    
    # 2. Twitter APIæ•°æ®æº
    bearer_token = os.getenv('TWITTER_BEARER_TOKEN')
    if bearer_token:
        try:
            twitter_api_source = TwitterAPISource(bearer_token=bearer_token)
            sources.append(twitter_api_source)
        except Exception as e:
            logging.warning(f"æ— æ³•åˆ›å»ºTwitter APIæ•°æ®æº: {e}")
    
    # 3. Apifyæ•°æ®æºï¼ˆå¦‚æœé…ç½®äº†ï¼‰
    apify_token = os.getenv('APIFY_API_TOKEN')
    apify_enabled = os.getenv('APIFY_ENABLE', 'false').lower() == 'true'
    if apify_enabled and apify_token:
        try:
            apify_source = ApifyTwitterSource(
                api_token=apify_token,
                actor_id=os.getenv('APIFY_ACTOR_ID', 'apidojo/tweet-scraper'),
                timeout=int(os.getenv('APIFY_TIMEOUT', '120'))
            )
            sources.append(apify_source)
        except Exception as e:
            logging.warning(f"æ— æ³•åˆ›å»ºApifyæ•°æ®æº: {e}")
    
    if not sources:
        raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„æ•°æ®æºï¼Œè¯·æ£€æŸ¥é…ç½®")
    
    # åˆ›å»ºæ•°æ®æºç®¡ç†å™¨
    data_manager = DataSourceManager(sources=sources)
    
    # åˆ›å»ºå¼‚æ­¥è¿è¡Œå™¨
    async_runner = AsyncRunner("data_updater")
    
    # åˆ›å»ºTwitteræœåŠ¡
    twitter_service = TwitterService(
        data_manager=data_manager,
        async_runner=async_runner
    )
    
    return twitter_service


async def quick_update_missing_fields(filter_recent: bool = True) -> UpdateResult:
    """å¿«é€Ÿæ›´æ–°ç¼ºå¤±å­—æ®µ - ä¾¿æ·æ–¹æ³•"""
    
    config = UpdaterConfig.create_safe_config()
    updater = await create_data_updater(config)
    
    result = await updater.update_all_records(
        filter_recent=filter_recent,
        batch_strategy="priority"  # ä¼˜å…ˆå¤„ç†ç¼ºå¤±å­—æ®µå¤šçš„è®°å½•
    )
    
    return result