"""
æ‰¹å¤„ç†ç®¡ç†å™¨

éµå¾ªSOLIDåŸåˆ™ï¼Œä¸“é—¨ç®¡ç†å¤§è§„æ¨¡æ•°æ®çš„æ‰¹é‡å¤„ç†
é’ˆå¯¹732æ¡è®°å½•çš„é«˜æ•ˆæ›´æ–°ç­–ç•¥
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional, Callable, AsyncGenerator
from dataclasses import dataclass
from datetime import datetime
import math

from ..database.models import CampaignTweetSnapshot
from ...core.config_factory import UpdaterConfig
from .rate_limiter import BatchRateLimiter


@dataclass
class BatchInfo:
    """æ‰¹æ¬¡ä¿¡æ¯ - æ•°æ®ç»“æ„"""
    batch_id: int
    start_index: int
    end_index: int
    size: int
    records: List[CampaignTweetSnapshot]
    priority: int = 0  # ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
    created_at: datetime = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()


@dataclass
class BatchResult:
    """æ‰¹æ¬¡å¤„ç†ç»“æœ"""
    batch_info: BatchInfo
    success_count: int
    failure_count: int
    errors: List[Dict[str, Any]]
    processing_time: float
    updated_records: List[CampaignTweetSnapshot]
    skipped_records: List[CampaignTweetSnapshot]


class BatchStrategy:
    """æ‰¹å¤„ç†ç­–ç•¥ - ç­–ç•¥æ¨¡å¼"""
    
    @staticmethod
    def create_equal_batches(records: List[CampaignTweetSnapshot], 
                           batch_size: int) -> List[BatchInfo]:
        """åˆ›å»ºç­‰å¤§å°æ‰¹æ¬¡"""
        batches = []
        total_records = len(records)
        
        for i in range(0, total_records, batch_size):
            end_index = min(i + batch_size, total_records)
            batch_records = records[i:end_index]
            
            batch_info = BatchInfo(
                batch_id=len(batches) + 1,
                start_index=i,
                end_index=end_index - 1,
                size=len(batch_records),
                records=batch_records
            )
            batches.append(batch_info)
        
        return batches
    
    @staticmethod
    def create_priority_batches(records: List[CampaignTweetSnapshot], 
                              batch_size: int,
                              priority_fn: Callable[[CampaignTweetSnapshot], int]) -> List[BatchInfo]:
        """åˆ›å»ºä¼˜å…ˆçº§æ‰¹æ¬¡"""
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_records = sorted(records, key=priority_fn)
        
        # åˆ›å»ºæ‰¹æ¬¡
        batches = []
        for i in range(0, len(sorted_records), batch_size):
            end_index = min(i + batch_size, len(sorted_records))
            batch_records = sorted_records[i:end_index]
            
            # è®¡ç®—æ‰¹æ¬¡ä¼˜å…ˆçº§ï¼ˆæœ€é«˜ä¼˜å…ˆçº§è®°å½•çš„ä¼˜å…ˆçº§ï¼‰
            batch_priority = min(priority_fn(record) for record in batch_records)
            
            batch_info = BatchInfo(
                batch_id=len(batches) + 1,
                start_index=i,
                end_index=end_index - 1,
                size=len(batch_records),
                records=batch_records,
                priority=batch_priority
            )
            batches.append(batch_info)
        
        # æŒ‰ä¼˜å…ˆçº§æ’åºæ‰¹æ¬¡
        batches.sort(key=lambda b: b.priority)
        
        # é‡æ–°åˆ†é…æ‰¹æ¬¡ID
        for idx, batch in enumerate(batches):
            batch.batch_id = idx + 1
        
        return batches
    
    @staticmethod
    def create_adaptive_batches(records: List[CampaignTweetSnapshot], 
                              base_batch_size: int,
                              complexity_fn: Callable[[CampaignTweetSnapshot], float]) -> List[BatchInfo]:
        """åˆ›å»ºè‡ªé€‚åº”å¤§å°æ‰¹æ¬¡ - æ ¹æ®å¤æ‚åº¦è°ƒæ•´æ‰¹æ¬¡å¤§å°"""
        batches = []
        current_batch = []
        current_complexity = 0.0
        max_complexity_per_batch = base_batch_size * 1.0  # åŸºå‡†å¤æ‚åº¦
        
        for record in records:
            record_complexity = complexity_fn(record)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼€å§‹æ–°æ‰¹æ¬¡
            if (len(current_batch) >= base_batch_size or 
                (current_batch and current_complexity + record_complexity > max_complexity_per_batch)):
                
                # åˆ›å»ºå½“å‰æ‰¹æ¬¡
                batch_info = BatchInfo(
                    batch_id=len(batches) + 1,
                    start_index=len(batches) * base_batch_size,  # ä¼°ç®—
                    end_index=len(batches) * base_batch_size + len(current_batch) - 1,
                    size=len(current_batch),
                    records=current_batch.copy()
                )
                batches.append(batch_info)
                
                # é‡ç½®å½“å‰æ‰¹æ¬¡
                current_batch = []
                current_complexity = 0.0
            
            current_batch.append(record)
            current_complexity += record_complexity
        
        # å¤„ç†æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch:
            batch_info = BatchInfo(
                batch_id=len(batches) + 1,
                start_index=len(batches) * base_batch_size,
                end_index=len(batches) * base_batch_size + len(current_batch) - 1,
                size=len(current_batch),
                records=current_batch
            )
            batches.append(batch_info)
        
        return batches


class BatchManager:
    """æ‰¹å¤„ç†ç®¡ç†å™¨ - å•ä¸€èŒè´£åŸåˆ™"""
    
    def __init__(self, 
                 config: UpdaterConfig,
                 rate_limiter: BatchRateLimiter = None):
        """
        åˆå§‹åŒ–æ‰¹å¤„ç†ç®¡ç†å™¨
        
        Args:
            config: æ›´æ–°å™¨é…ç½®
            rate_limiter: é€Ÿç‡é™åˆ¶å™¨
        """
        self.config = config
        self.rate_limiter = rate_limiter or BatchRateLimiter(
            requests_per_minute=config.requests_per_minute,
            requests_per_hour=config.requests_per_hour,
            base_delay=config.request_delay_seconds,
            batch_delay=config.batch_delay_seconds
        )
        
        self.logger = logging.getLogger(__name__)
        
        # æ‰¹å¤„ç†ç»Ÿè®¡
        self._total_batches = 0
        self._processed_batches = 0
        self._failed_batches = 0
        self._total_records = 0
        self._processed_records = 0
        self._failed_records = 0
        
        # çŠ¶æ€ç®¡ç†
        self._is_running = False
        self._is_paused = False
        self._should_stop = False
        
    def create_batches(self, 
                      records: List[CampaignTweetSnapshot],
                      strategy: str = "equal") -> List[BatchInfo]:
        """åˆ›å»ºæ‰¹æ¬¡ - å·¥å‚æ–¹æ³•"""
        
        if not records:
            return []
        
        self.logger.info(f"ğŸ“¦ åˆ›å»ºæ‰¹æ¬¡: {len(records)} æ¡è®°å½•, ç­–ç•¥: {strategy}")
        
        if strategy == "equal":
            batches = BatchStrategy.create_equal_batches(records, self.config.batch_size)
            
        elif strategy == "priority":
            def priority_function(record: CampaignTweetSnapshot) -> int:
                """ä¼˜å…ˆçº§å‡½æ•° - ç¼ºå¤±å­—æ®µè¶Šå¤šä¼˜å…ˆçº§è¶Šé«˜"""
                priority = 0
                if not record.author_name:
                    priority += 10  # ç¼ºå¤±ä½œè€…åæœ€é‡è¦
                if not record.tweet_time_utc:
                    priority += 5   # ç¼ºå¤±æ—¶é—´è¾ƒé‡è¦
                return priority
                
            batches = BatchStrategy.create_priority_batches(
                records, self.config.batch_size, priority_function)
            
        elif strategy == "adaptive":
            def complexity_function(record: CampaignTweetSnapshot) -> float:
                """å¤æ‚åº¦å‡½æ•° - é¢„ä¼°å¤„ç†å¤æ‚åº¦"""
                complexity = 1.0  # åŸºç¡€å¤æ‚åº¦
                
                # ç¼ºå¤±å­—æ®µè¶Šå¤šè¶Šå¤æ‚
                missing_fields = 0
                if not record.author_name:
                    missing_fields += 1
                if not record.tweet_time_utc:
                    missing_fields += 1
                
                complexity += missing_fields * 0.5
                
                # æ¨æ–‡ç±»å‹å½±å“å¤æ‚åº¦
                if record.tweet_type in ['quote', 'reply']:
                    complexity += 0.3
                    
                return complexity
                
            batches = BatchStrategy.create_adaptive_batches(
                records, self.config.batch_size, complexity_function)
        else:
            raise ValueError(f"æœªçŸ¥çš„æ‰¹å¤„ç†ç­–ç•¥: {strategy}")
        
        self._total_batches = len(batches)
        self._total_records = len(records)
        
        self.logger.info(f"âœ… æ‰¹æ¬¡åˆ›å»ºå®Œæˆ: {len(batches)} ä¸ªæ‰¹æ¬¡")
        
        # æ‰“å°æ‰¹æ¬¡åˆ†å¸ƒç»Ÿè®¡
        batch_sizes = [batch.size for batch in batches]
        avg_size = sum(batch_sizes) / len(batch_sizes) if batch_sizes else 0
        min_size = min(batch_sizes) if batch_sizes else 0
        max_size = max(batch_sizes) if batch_sizes else 0
        
        self.logger.info(f"ğŸ“Š æ‰¹æ¬¡ç»Ÿè®¡: å¹³å‡å¤§å° {avg_size:.1f}, èŒƒå›´ {min_size}-{max_size}")
        
        return batches
    
    async def process_batches(self, 
                            batches: List[BatchInfo],
                            batch_processor: Callable[[BatchInfo], AsyncGenerator[BatchResult, None]],
                            progress_callback: Optional[Callable[[int, int, BatchResult], None]] = None) -> List[BatchResult]:
        """
        å¤„ç†æ‰¹æ¬¡é˜Ÿåˆ—
        
        Args:
            batches: æ‰¹æ¬¡åˆ—è¡¨
            batch_processor: æ‰¹æ¬¡å¤„ç†å™¨ï¼ˆå¼‚æ­¥ç”Ÿæˆå™¨ï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            æ‰¹æ¬¡å¤„ç†ç»“æœåˆ—è¡¨
        """
        if not batches:
            return []
        
        self.logger.info(f"ğŸš€ å¼€å§‹å¤„ç† {len(batches)} ä¸ªæ‰¹æ¬¡...")
        
        self._is_running = True
        self._should_stop = False
        self._processed_batches = 0
        self._failed_batches = 0
        self._processed_records = 0
        self._failed_records = 0
        
        results = []
        
        try:
            # å¦‚æœæ”¯æŒå¹¶å‘å¤„ç†
            if self.config.max_concurrent_batches > 1:
                results = await self._process_batches_concurrent(
                    batches, batch_processor, progress_callback)
            else:
                results = await self._process_batches_sequential(
                    batches, batch_processor, progress_callback)
                    
        except Exception as e:
            self.logger.error(f"âŒ æ‰¹æ¬¡å¤„ç†å¼‚å¸¸: {e}")
            raise
        finally:
            self._is_running = False
        
        # è¾“å‡ºæ€»ç»“
        total_success = sum(r.success_count for r in results)
        total_failure = sum(r.failure_count for r in results)
        total_time = sum(r.processing_time for r in results)
        
        self.logger.info(f"âœ… æ‰¹æ¬¡å¤„ç†å®Œæˆ!")
        self.logger.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        self.logger.info(f"   æ‰¹æ¬¡: {self._processed_batches}/{self._total_batches}")
        self.logger.info(f"   è®°å½•: æˆåŠŸ {total_success}, å¤±è´¥ {total_failure}")
        self.logger.info(f"   æ—¶é—´: {total_time:.1f}s")
        
        return results
    
    async def _process_batches_sequential(self, 
                                        batches: List[BatchInfo],
                                        batch_processor: Callable,
                                        progress_callback: Optional[Callable] = None) -> List[BatchResult]:
        """é¡ºåºå¤„ç†æ‰¹æ¬¡"""
        results = []
        
        for i, batch in enumerate(batches):
            if self._should_stop:
                self.logger.info("ğŸ“¢ æ¥æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»ˆæ­¢å¤„ç†")
                break
            
            # æ£€æŸ¥æš‚åœçŠ¶æ€
            while self._is_paused and not self._should_stop:
                self.logger.info("â¸ï¸  å¤„ç†å·²æš‚åœï¼Œç­‰å¾…æ¢å¤...")
                await asyncio.sleep(1)
            
            if self._should_stop:
                break
            
            self.logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {i+1}/{len(batches)} (ID: {batch.batch_id}, å¤§å°: {batch.size})")
            
            try:
                # æ‰¹æ¬¡é—´å»¶è¿Ÿæ§åˆ¶
                if i > 0:  # ç¬¬ä¸€ä¸ªæ‰¹æ¬¡ä¸éœ€è¦ç­‰å¾…
                    await self.rate_limiter.wait_for_batch()
                
                # å¤„ç†æ‰¹æ¬¡
                start_time = asyncio.get_event_loop().time()
                
                # ä½¿ç”¨å¼‚æ­¥ç”Ÿæˆå™¨å¤„ç†æ‰¹æ¬¡
                batch_result = None
                async for result in batch_processor(batch):
                    batch_result = result
                    break  # åªå–ç¬¬ä¸€ä¸ªç»“æœ
                
                if batch_result is None:
                    # åˆ›å»ºå¤±è´¥ç»“æœ
                    batch_result = BatchResult(
                        batch_info=batch,
                        success_count=0,
                        failure_count=batch.size,
                        errors=[{"error": "æ‰¹æ¬¡å¤„ç†å™¨æ²¡æœ‰è¿”å›ç»“æœ"}],
                        processing_time=0.0,
                        updated_records=[],
                        skipped_records=batch.records
                    )
                
                end_time = asyncio.get_event_loop().time()
                batch_result.processing_time = end_time - start_time
                
                results.append(batch_result)
                
                # æ›´æ–°ç»Ÿè®¡
                self._processed_batches += 1
                self._processed_records += batch_result.success_count
                self._failed_records += batch_result.failure_count
                
                if batch_result.failure_count > 0:
                    self._failed_batches += 1
                
                # è¿›åº¦å›è°ƒ
                if progress_callback:
                    progress_callback(i + 1, len(batches), batch_result)
                
                # è®°å½•å¤„ç†ç»“æœ
                success_rate = batch_result.success_count / batch.size * 100 if batch.size > 0 else 0
                self.logger.info(f"âœ… æ‰¹æ¬¡ {batch.batch_id} å®Œæˆ: {batch_result.success_count}/{batch.size} æˆåŠŸ ({success_rate:.1f}%)")
                
                if batch_result.errors:
                    self.logger.warning(f"âš ï¸  æ‰¹æ¬¡ {batch.batch_id} æœ‰ {len(batch_result.errors)} ä¸ªé”™è¯¯")
                
            except Exception as e:
                self.logger.error(f"âŒ æ‰¹æ¬¡ {batch.batch_id} å¤„ç†å¤±è´¥: {e}")
                
                # åˆ›å»ºé”™è¯¯ç»“æœ
                error_result = BatchResult(
                    batch_info=batch,
                    success_count=0,
                    failure_count=batch.size,
                    errors=[{"error": str(e), "batch_id": batch.batch_id}],
                    processing_time=0.0,
                    updated_records=[],
                    skipped_records=batch.records
                )
                results.append(error_result)
                
                self._failed_batches += 1
                self._failed_records += batch.size
                
                # è¿ç»­å¤±è´¥æ£€æŸ¥
                consecutive_failures = sum(1 for r in results[-self.config.max_consecutive_failures:] 
                                         if r.success_count == 0)
                
                if consecutive_failures >= self.config.max_consecutive_failures:
                    self.logger.error(f"âŒ è¿ç»­ {consecutive_failures} ä¸ªæ‰¹æ¬¡å¤±è´¥ï¼Œæš‚åœå¤„ç†")
                    await asyncio.sleep(self.config.pause_on_error_seconds)
        
        return results
    
    async def _process_batches_concurrent(self, 
                                        batches: List[BatchInfo],
                                        batch_processor: Callable,
                                        progress_callback: Optional[Callable] = None) -> List[BatchResult]:
        """å¹¶å‘å¤„ç†æ‰¹æ¬¡"""
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(self.config.max_concurrent_batches)
        results = []
        completed_count = 0
        
        async def process_single_batch(batch: BatchInfo, index: int) -> BatchResult:
            nonlocal completed_count
            
            async with semaphore:
                if self._should_stop:
                    return None
                
                try:
                    self.logger.info(f"ğŸ“¦ å¹¶å‘å¤„ç†æ‰¹æ¬¡ {index+1}/{len(batches)} (ID: {batch.batch_id})")
                    
                    start_time = asyncio.get_event_loop().time()
                    
                    # å¤„ç†æ‰¹æ¬¡
                    batch_result = None
                    async for result in batch_processor(batch):
                        batch_result = result
                        break
                    
                    if batch_result is None:
                        batch_result = BatchResult(
                            batch_info=batch,
                            success_count=0,
                            failure_count=batch.size,
                            errors=[{"error": "æ‰¹æ¬¡å¤„ç†å™¨æ²¡æœ‰è¿”å›ç»“æœ"}],
                            processing_time=0.0,
                            updated_records=[],
                            skipped_records=batch.records
                        )
                    
                    end_time = asyncio.get_event_loop().time()
                    batch_result.processing_time = end_time - start_time
                    
                    completed_count += 1
                    
                    # è¿›åº¦å›è°ƒ
                    if progress_callback:
                        progress_callback(completed_count, len(batches), batch_result)
                    
                    return batch_result
                    
                except Exception as e:
                    self.logger.error(f"âŒ å¹¶å‘æ‰¹æ¬¡ {batch.batch_id} å¤±è´¥: {e}")
                    return BatchResult(
                        batch_info=batch,
                        success_count=0,
                        failure_count=batch.size,
                        errors=[{"error": str(e)}],
                        processing_time=0.0,
                        updated_records=[],
                        skipped_records=batch.records
                    )
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = [process_single_batch(batch, i) for i, batch in enumerate(batches)]
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        task_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        for result in task_results:
            if isinstance(result, Exception):
                self.logger.error(f"âŒ ä»»åŠ¡å¼‚å¸¸: {result}")
                continue
            
            if result is not None:
                results.append(result)
                
                # æ›´æ–°ç»Ÿè®¡
                self._processed_batches += 1
                self._processed_records += result.success_count
                self._failed_records += result.failure_count
                
                if result.failure_count > 0:
                    self._failed_batches += 1
        
        return results
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æ‰¹å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        processed_rate = (self._processed_batches / self._total_batches * 100) if self._total_batches > 0 else 0
        success_rate = (self._processed_records / (self._processed_records + self._failed_records) * 100) if (self._processed_records + self._failed_records) > 0 else 0
        
        return {
            'status': {
                'is_running': self._is_running,
                'is_paused': self._is_paused,
                'should_stop': self._should_stop
            },
            'batches': {
                'total': self._total_batches,
                'processed': self._processed_batches,
                'failed': self._failed_batches,
                'remaining': self._total_batches - self._processed_batches,
                'processed_percentage': processed_rate
            },
            'records': {
                'total': self._total_records,
                'processed_successfully': self._processed_records,
                'failed': self._failed_records,
                'success_rate': success_rate
            },
            'rate_limiter': self.rate_limiter.get_statistics() if self.rate_limiter else None
        }
    
    def pause(self):
        """æš‚åœå¤„ç†"""
        self._is_paused = True
        self.logger.info("â¸ï¸  æ‰¹å¤„ç†å·²æš‚åœ")
    
    def resume(self):
        """æ¢å¤å¤„ç†"""
        self._is_paused = False
        self.logger.info("â–¶ï¸  æ‰¹å¤„ç†å·²æ¢å¤")
    
    def stop(self):
        """åœæ­¢å¤„ç†"""
        self._should_stop = True
        self.logger.info("â¹ï¸  æ‰¹å¤„ç†åœæ­¢ä¿¡å·å·²å‘é€")
    
    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self._processed_batches = 0
        self._failed_batches = 0
        self._processed_records = 0
        self._failed_records = 0
        self.logger.info("ğŸ”„ ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")